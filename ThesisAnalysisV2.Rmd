---
title             : "Within-person replicability of idiographic networks"
shorttitle        : ""

author: 
  - name          : "Ricarda K. K. Proppert"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : ""
    email         : "ricarda.proppert@gmail.com"

affiliation:
  - id            : "1"
    institution   : "Clinical Psychology, Leiden University, The Netherlands"


abstract: |
  Evidence-based mental health programs have long conceptualized mental disorders in terms of interactions between thoughts, feelings, behaviours and external factors.
  
  Idiographic network models are a relatively novel way of modeling such intra-individual psychological processes. These methods are not without limitations, and concerns have been raised about the stability and accuracy of estimated networks.
  
  While methods to assess network parameter accuracy have been developed for cross-sectional data, no such method exists for single-subject data. The extend to which idiographic networks are stable, or vary over time, is unknown.
  
  In the current work, we reanalyse daily symptom records of people with personality disorders to explore the stability of idiographic networks over time, as well as the degree to which network stability varies across individuals. We further explore antecents that may relate to inter-individual variation in network stability using predictive LASSO regression.

  
keywords          : ""
wordcount         : ""


floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_doc

bibliography: references.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = TRUE, message = TRUE)
# install.packages("here")
# setwd("~/GitHub/thesis_repo")
library("here")
here::i_am("ThesisAnalysisV2.Rmd") # find directory where file is in
# install.packages("conflicted")
library("conflicted")
conflict_prefer("filter", "dplyr")
# install.packages("tidyverse")
library("tidyverse")
# install.packages("papaja")
library("papaja")
# install.packages("imputeTS")
 library("imputeTS")
# install.packages("graphicalVAR")
library("graphicalVAR")
# install.packages("qgraph")
library("qgraph")

# r_refs("r-references.bib")

# Notation: 
# ---------
#     data:   full data set. Operations on the complete data set ()
#             In chunks that manipulate the whole data set (eg., imputation, detrending) I reassign the manipulated data object to 'data', so that results can easily be rerun on not detrended / not imputed data by simply not evaluating the previous code chunk
#     p:      participant (iteration in loop)
#     v:      variable (iteration in loop)
#     d:      temporary data object in loop


# sessionInfo()
# -------------
# R version 4.0.3 (2020-10-10)
# Platform: x86_64-apple-darwin17.0 (64-bit)
# Running under: macOS Big Sur 10.16
```

```{r readData, message=TRUE, warning=TRUE}
# read data
# data <- read.csv(here("data", "dd100-proppert.csv"))
# 
# N <- data$participantID %>% unique()
# range(N) # 1 : 116
# length(unique(N)) # 112
# setdiff(1:116, N) # Participants  8, 25, 45, 92  had been excluded by original authors because more than 30% of responses were missing
# 
# # rename IDs for easier looping
# data$participantID <- factor(
#   data$participantID, 
#   levels = unique(data$participantID), 
#   labels = 1:length(unique(data$participantID))
#   ) %>% 
#   as.numeric()
# 
# #select variables I want to use, save to publish along code
# data_daily    <- data %>% select(participantID:dpds32) %>% select(-sleep, -alcohol.daily, -drugs.daily, -starts_with("stress")) # daily
# data_baseline <- data %>% select(participantID, assessment.date:age, happy, swlsMean, recentTreatment, neoN:neoC) %>% distinct()
# write.csv(data_daily, here("data", "dd100_daily.csv"))
# write.csv(data_baseline, here("data", "dd100_baseline.csv"))
# rm(data)

data_daily    <- read.csv(here("data", "dd100_daily.csv"))    %>% select(-X)
data_baseline <- read.csv(here("data", "dd100_baseline.csv")) %>% select(-X)

```

<!-- TODO: 
-    add Eiko as Supervisor
- place authornote in YAML again, currently throws error due to papaja bug. 
Issue: https://github.com/crsh/papaja/issues/225

authornote: "Master thesis for the Research Master Clinical- and Health psychology (combined track) under supervision of Dr. Eiko Fried. We thank Dr. Aidan Wright, Pittsburg University, and his colleagues for sharing their data with us. The reproducible analysis script can be downloaded at "link Github", data are available upon request."

- Abstract needs Results and Implication
- Specify authorship roles in YAML (see Master guidelines)
- wording consistent: stability vs accuracy
-->```
# Introduction

## Idiographic psychological networks

Idiographic network models are of growing interest to clinical psychology because they may address two recently voiced calls in clinical psychology: Firstly, there seems to be a need for psychological research to re-orient towards idiographic methods that study intra-individual processes as opposed to group-level differences [@Molenaar2004].
<!--TODO mention that results from grouplevel may not generalize to individual level, eg. @Bulteel2018 --> Secondly, scholars are proposing a paradigm shift away from reductionism towards studying the complexity of psychological phenomena.
The Network theory of mental disorders [@BorsboomCramer2013; @CramerEtAl2010] attempts to integrate psychology with insights and methods from complexity science, proposing a novel but well-received theoretical framework to study and understand the underpinnings of psychopathology.
The network theory of mental disorders conceptualizes psychopathology as an emergent state of dynamically interacting psychological symptoms and states, as well as factors external to this system.
<!-- Eiko: can also be somatic sx, but R can't find way to keep sentence readible--> This account seems closely aligned with established clinical practices where informal case conceptualizations in form of path diagrams are used to describe the proposed mechanisms of a given disorder (@BurgerEtAl2020, @ScholtenEtAl2020).
<!--For example, dysfunctional cycle of mutually interacting stressors, thoughts, feelings and behaviors.-->

With the ongoing development of statistical methods to visualize and quantify such system's structures and behaviors, called psychological networks [@EpskampEtAl2018], an increasing body of research is now applying this framework to study symptom networks in people with mental disorders.
<!-- ref Sascha Diss, and perhaps mention / explain "psychonetrics"--> Psychological networks consist of elements (nodes) and their pairwise interactions (edges), together forming a complex system.
Nodes represent psychological or other variables, such as psychological or somatic symptoms, stressors, or behaviors.
Edges represent pairwise relationships between these variables.
These relationships may be directed or undirected, positive or negative, and can differ in strength.
These associations and are thought to result from of a multitude of mechanisms which may not necessarily be known or defined.


<!-- talk about stationarity assumption and time-varying autoregressive models eg HaslbeckBringmanWaldop2018, 
and https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7186258/ -->

## Temporal stability of networks

Most of the early psychological network research focused on group-level analyses and cross-sectional comparisons, mainly for practical reasons such as availability of existing data-sets.
Few studies have investigated longitudinal network stability at a group level (e.g., in veterans pre- to post-combat, @SegalEtAl2020; military veterans, @vonStockertEtAl2018; among adolescent earthquake survivors, @GeEtAl2019; and in patients with Anxiety and Depression, @CurtissEtAl ).

Besides group-level analyses, there is growing interest in idiographic network models of intra-individual symptom dynamics.
Researchers in clinical psychology in particular hope that such models could resolve what is known as the Therapist's dilemma <!-- explain and ref --> (eg., @FrumkinEtAl2020, @HoweEtAl2020, @CavigliaColeman2016, @HoffartJohnson2020).
In clinical psychology research, not only momentary network structures are of interest, but especially *changes* in network structure over time.
For example, changes in network structure may indicate therapeutic progress [@ThononEtAl2020] or relapse [@WichersEtAl2016].
Even subtle changes in network structure are of interest, as they are thought to act as potential early warning signals which may predict future major change, i.e. a system's phase transition from a healthy attractor state to a disordered one.
For example, signs of critical slowing down, showing as increased auto-correlations and variances of items, have been demonstrated to signal a patient's relapse into depression upon stopping antidepressant treatment (@WichersEtAl2016; for similar work on resilience, see @KuranovaEtAl2020).

<!-- mention case formulation paper burger 2020 -->

<!-- feasibility study frumkin -->

<!-- Stability and accuracy of psychological networks TODO: explain vocab as used in this work. Accuracy = accuracy of parameter estimates Stability = stability of underlying process -->

<!--In practice, these two are rarely distinguished. A major reason for this is that methods to estimate network parameter accuracy have only recently become available and implemented in popular software packages (estimating stability paper) Furthermore, for the most popular estimation method graphical Var / ml Var (vector autoregressive models) they are not available yet. -->

Empirical work is published in which differences in network structures are interpreted at face value.
For example, @ThononEtAl2020 followed three psychiatric patients over the course of treatment and interpreted changes in idiographic network structures over time as additional evidence for and a description of the observed therapeutic change.
Such a substantial interpretation of network instability hinges on the assumption that differences in estimated network parameters accurately reflect change in the data-generating mechanism.
This assumption rests on two conditions: First, parameter estimates need to be an accurate reflection of the true underlying relationship.
Second, the underlying mechanism would have remained stable if no intervention took place (related to the assumption of stationarity made during model estimation).
Neither of these conditions can currently be evaluated in rigorous ways for idiographic network models.
Methods to assess the accuracy of parameter estimates have been developed for group-level estimation procedures [@EpskampEtAl2018], but are not yet available for idiographic estimation methods.
<!-- explain estimating accuracy methods and why not available for gVAR and idiographic models --> The degree to which underlying dynamic psychological processes are stable within individuals over time is unclear.
Current literature investigating the temporal stability of idiographic networks mostly focuses on settings where change is expected to occur, e.g., in response to psychological treatment [@ThononEtAl2020], discontinuation of antidepressant medication [@WichersEtAl2016], or the COVID-19 pandemic [@BeckJackson2021].
To our knowledge, only one study has investigated the temporal stability of psychological networks in situations where no change would be expected.
@BeckJackson2020a investigated the consistency of idiographic personality over the course of two years.
They reported high consistency among contemporaneous associations, low consistency of temporal associations, and considerable interpersonal variability in the stability of networks.

<!-- BeckJackson2020 use GIMME and no detrending, super helpful prereg and code on osf + gh -->

<!--
## Current challenges in research on idiographic networks

Explain estimation procedures and assumptions in more detail: - explain graphicalVAR estimation - stationarity - multivariate normality - no measurement error - mention related methods that we do not use here, and why (mlVAR, baysian, all the other not so networky idioraphic models (see idio review)

-   require high power, which is difficult to obtain in longitudinal N=1 study (burdensome)

-   even if power is high and networks can be estimated reliably, longitudinal stability of networks is unknown (ie how they generalize to different time within same individual)

-   stability of estimates is an important issue (beyond scopoe here but explain this as important limitation)

-   even if estimates stable, replicability not warranted:

-   How stable is the data generating process we try to capture? Is this the process of interest? Is it malleable? Phase transitions?

-   feasibility
-->

<!-- idea eiko: "There are 5 main challenges, bla bla bla. In this work, we will tacke 1,2  and 3". Rest goes into limitations.

"where ema res is by nature confined to assess snapshots we dont now how well networks replicate within a person even
"
q of replicability "empirical replicability"  vs statistical replicability (simulation studies give good evidence) - see forbes

STATIONARITY: Eiko says it's univariate, so (means and variances, so 1st and 2nd order only)

### Text Box: List of most relevant factors influencing "snapshottiness" of idio networks
List statistical and substantial reasons
response shift bias
measurement error
stationartiy
§change"
motivation
response time? as potential covariate?

statistical things:
- sparsity (
- density: strength of sum of edges
- sparsity:  "percentage non-zero cells" 
-->

## Aim of this study

<!-- A recent simulation study (preprint Mansueto), ... required power would be bla.-->

The present study aims to assess the stability of idiographic networks of psychopathology, as well as exploring potential person-specific factors and statistical factors predicting interpersonal variation in temporal network stability.

We will address the following research questions:

RQ1: How stable are estimated idiographic network structures over time?

RQ2: What factors, person-specific or of statistical nature, predict inter-individual variation in intra-individual replicability?

To this end, we re-analyze daily diary data collected in a clinical sample of people diagnosed with a personality disorder [@WrightEtAl2015a; @WrightSimms; @WrightEtAl2015].
Participants (N=116) provided once-daily ratings of their mood, behaviors, and daily stressors over the course of 100 consecutive days.
To assess intra-individual stability of idiographic networks, we will fit separate idiographic networks on the first and last 50 days of measurement.
Network structures of each individual's first vs. last 50 days of measurements will be compared by correlating estimated path estimates, with high correlation indicating high temporal stability.
Finally, person-specific antecedents and statistical factors will be tested for their ability to predict interindividual variation in network stability using predictive LASSO regression.

# Methods
## Data Set

A summarized version of the data are published HERE, an overview of all available variables can be found in Appendix A.

The original study design by which the data were collected are described in detail in previous publications [@WrightEtAl2015; @WrightSimms; @WrightEtAl2015a].
The study investigated daily dynamics in affect, stress, andexpressions of personality disorder, along with some lifestyle variables such as self-reported sleep, drug and alcohol use, and overall functioning.
Participants were recruited from an ongoing clinical study (N=628, @SimmsEtAl2011) that targeted individuals who had received psychiatric treatment within the previous two years, recruited via flyers distributed at mental health clinics across Western New York, USA. They were invited for study participation if they met the diagnostic requirements of any personality disorder during the initial structured clinical interview conducted for the parent study (SCID-II, TODO REF), and if they had daily internet access via a computer or mobile device.
116 participants were initially enrolled, of which 101 participants completed at least 30 of the desired 100 daily measurements.

Participants completed daily measurements over the course of 100 consecutive days.
Participants completed surveys at roughly the same time each night, depending on their individual schedule.
Therefore, therefore, this data set is expected to meet the assumption of roughly equal time intervals in between measurements [@Epskamp2020].
Furthermore, the original authors reported stability of means and variances of expressions of daily psychopathology in @WrightSimms, indicating that the assumption of stationarity may be realistic [@Epskamp2020].
Participants started measurements asynchronically, so that no sample-wide history effects should affect our results on longitudinal within-person stability of estimated networks.

<!-- We chose this data set for this study for three main reasons: It met our minimum requirement of spanning a sufficiently long time scale of intensive longitudinal data, it was readily available, and we believe the design to be a good representation of a feasible clinical setting: Daily records are an already well-established method for enhancong diagnosis and monitoring patients daily lives, and asks less burden of participants compared to more intensive EMA methods. Secondly, the duration of 100-days consistent data collection was long enough so that splitting the data seemed feasible. Recorded variables - although being collected in a sample of people diagnosed with a personality disorder - were general enough to not be restricted to a certain mental disorder. -->

<!--We believe this data set to be representative of a clinically realistic sampling scheme for two reasons: Firstly, measuring symptoms daily as opposed to more often (eg. 4 times daily, as often used in ESM), is an already established method to enhance clinical diagnosis of certain disorders (find info), and even used as therapeutic technique in evidence-based clinical treatments (tracking mood to better understand ones emotional life, find source?). Secondly, daily measurements with a timescale of 1 day in between measuements may be desirable for modeling variables that typically fluctuate on a daily level only and are more within the control of the patient, such as sleep or exercise. Furthermore, the time frame of 100 days may strike a balance of being at the lower end of what may be needed for sufficient statistical power, and at the higher end of what may be an acceptable burden to pose on clinically distressed patients, as well as realistic for typical treatment durations.-->

<!-- Representativeness of realistic clinical designs: Applications of idiographic network models are currently scarce, mainly because they require well-powered data that necessarily come at increased burden to the participants. To improve power in single-subject longitudinal designs, such as ESM / EMA or daily diary studies, Participants are required to either provide more intensive momentary data, or provide less intensive (eg., daily) records over a longer period. Specific power requirements for idiographic networks are still unknown. Some simulation work suggests that ..... , depending on the number of estimated nodes and amount of missingness. Given that daily diaries a more established method of data collection in clinical research and mental health care, and assuming that it poses less burden on participants compared to more intensive sampling frames, we hope that this data is at the higher end of what seems realistic and feasible for future study designs and applications. Simultaneously, although no precise power estimates are currently available, we hope that meets the minimum power needed for our analyses (up to 50 data points per individual, per network). -->

## Data pre-processing:

###  Participant exclusion:

Participants with more than 30 days of non-response, or more than 15 in either the first or last 50 days of measurement, will be excluded from the analyses.
Remaining missing data points of daily diary variables will be imputed using the Kalman Filter [@Harvey1989] , which has been shown to perform well in simulation studies [@MansuetoEtAl2020].

```{r exludeMissing, message=TRUE, warning=TRUE}

data <- data_daily

# count nr of missing per px
idx_missing <- matrix(NA, N, 7)
colnames(idx_missing) <- c("ID", "CompletedTotal", "CompletedT1", "CompletedT2", "Include", "Var_Tasks_T1", "Var_Tasks_T2")

for (p in 1:N){
  # ID
  idx_missing[p,1] <- p
  # CompletedTotal 
  d <- filter(data, data$participantID == p)
  idx_missing[p,2] <-  unique(d$total.days)
  # CompletedT1 
  d1 <-  filter(d, day.response %in% 1:50, missingResponse == 1)
  idx_missing[p,3] <- dim(d1)[1]
  # CompletedT2
  d2 <-  filter(d, day.response %in% 51:100, missingResponse == 1)
  idx_missing[p,4] <- dim(d2)[1]
  # also exclude participants with zero variance in 'tasks' variable
  d3 <- filter(d, day.response %in% 1:50)
  idx_missing[p,6] <- var(d3$tasks, na.rm = T) > 0
  d4 <- filter(d, day.response %in% 51:100)
  idx_missing[p,7] <- var(d4$tasks, na.rm = T) > 0
  # Include; TRUE if more than 70 responses overall, and more than 35 both T1 and T2, and non-zero var in 'tasks'
  idx_missing[p,5] <- idx_missing[p,2]>70 &
                      idx_missing[p,3]>35 &
                      idx_missing[p,4]>35 &  
                      idx_missing[p,6]==T & 
                      idx_missing[p,7]==T
}

# included IDs
idx_included <- idx_missing %>% as.data.frame() %>% filter(Include == TRUE)

# exclude participants
data <- filter(data, participantID %in% idx_included$ID)

# rename IDs for easy looping
data$participantID <- factor(data$participantID, 
                      levels = unique(data$participantID), 
                      labels = 1:length(unique(data$participantID))) %>% 
                      as.numeric()

# N: nr participants retained
N <- max(unique(data$participantID)) # 79

data$PosAffect <- NA
data$NegAffect <- NA
data$StressSev <- NA
data_daily_raw <- data # keep copy of data after exclusion of participants
```


#### Missing Data Imputation

```{r Kalman, message=TRUE, warning=TRUE}
#   apply Kalman filter to impute at level of participant, based on full 100d
emaLabels <- data %>% select(tasks:dpds32) %>% colnames()

library("imputeTS")
for (p in 1:N){
    for (v in seq_along(emaLabels)){
      if (length(unique(na.omit(data[data$participantID == p, 5+v]))) > 1){
        data[data$participantID == p, 5+v] <- na_kalman(data[data$participantID == p, 5+v]) # throws error, see Note below
        # cat("<- T ","ID",p,"var",v, sep = "")   # for check
      } else  data[data$participantID == p, 5+v] <- unique(na.omit(data[data$participantID == p, 5+v]))
        # cat("<- F","ID",p,"var",v, sep = "")    # for check
    }
}

# compute composite variables for positive affect, negative affect, and stress severity
data$PosAffect <- (data$alert + data$active + data$attentive + 
                   data$determined + data$inspired) / 5
data$NegAffect <- (data$afraid + data$nervous + data$hostile + 
                   data$ashamed + data$upset) / 5
data$StressSev <- (data$severe1 + data$severe2 + data$severe3 +
                   data$severe4 + data$severe5 + data$severe6 + data$severe7) / 7
emaLabels <-  data %>% 
              select(-(participantID:total.days)) %>% 
              colnames()

data_daily_imputed <- data # keep copy of imputed data
# TODO: reverse code positive affect for better visual interpretability? 

# NOTE:
# error is thrown when assigning imputed data to df:
# 
# possible convergence problem: 'optim' gave code = 52 
# and message ‘ERROR: ABNORMAL_TERMINATION_IN_LNSRCH’            [,1]      [,2]
# 
# I checked the iterations with errors manually, the imputation and assignment seem to work despite error, so I continue.
# 
# example of itiration w errors: 
# i = 86; v = 41
# cbind( data_daily_imputed[data_daily_imputed$participantID == p, 5+v] , na_kalman(data[data_daily_imputed$participantID == p, 5+v]))
```

#### Detrending linear effects

Variables used for network estimation will be linearly detrended across the 100 day time course, at the level of the individual.
<!-- chose individual bec people are not measured at the same time --> <!-- no detrending for 2 reasons: a) not many study report doing it, the two studies most closely related to mine don't either (Beck Github), and b) I don't think I / we understand how it relates to stationarity vs change in network structure, so I rather not either detrend different time effects in T1 vs T2, vs detrend across T1 and T2 and potentially inflate network similarity bec. why would I in one instance assume the true process is stationary, and in another investigate whether it isn't? HOWEVER Wichers2016 critical transition paper DID detrend and apparently it didnt affect results... mhh. CODE: see email eiko, in dropbox, no attachment-->

```{r detrending, eval=, message=TRUE, warning=TRUE}

# We detrend at level of participant, using full 100d, regardless of significance level of linear trend
data <- data_daily_imputed
for (p in 1:N) {
  for (v in seq_along(emaLabels)){
    if ( data[data$participantID == p, 5+v] %>% 
         is.na() %>% 
         sum() == 
         length(data[data$participantID == p, 5+v])) {next} #TODO: find more elegant solution to not evaluate composite scores...
    formula <- as.formula(paste0(emaLabels[v], " ~ 1 + day.response")) # define formula for linear trend
    trend_v <- lm(formula, data = data[data$participantID == p, ]) # fit model per person per var
    data[data$participantID == p, 5+v] <- residuals(trend_v) # detrend 
  }
}
data_detrended <- data # keep copy of data after detrending

# NOTE: 
# To check for significance of alpha, include below code in loop: 
# if (anova(trend_v)[["Pr(>F)"]][1] < 0.05) { ... } else next

```

```{r descriptives, message=TRUE, warning=TRUE}

# prepare data frame to store variable descriptives per person per time period
EMA_descriptives <- data.frame(
  ID = rep(c(1:N), each = length(emaLabels)),
  Var = rep(seq_along(emaLabels), N),
  emaLabels = rep(c(emaLabels), N)
  )

# function to filter rows for T1 and T2 data
.filterData <- function(timeperiod = c("T1", "T2"), dat, p){
  if (timeperiod == "T1"){
    t <- 1:50
  } else if ( timeperiod == "T2"){
    t <- 51:100    
  } else t <- 1:100
  return( filter(dat, participantID == p, day.response %in% t) )
}

# extract variable descriptives
for (p in 1:N){
    raw_T1        <- .filterData("T1", data_daily_raw, p) 
    raw_T2        <- .filterData("T2", data_daily_raw, p)
    imputed_T1    <- .filterData("T1", data_daily_imputed, p)
    imputed_T2    <- .filterData("T2", data_daily_imputed, p)
    detrended_T1  <- .filterData("T1", data_detrended, p)
    detrended_T2  <- .filterData("T2", data_detrended, p)
  for (v in seq_along(emaLabels)){
    # dynamic index for EMA_descriptives df
    idx <- (p-1)*length(emaLabels)+v
    # proportion completed T1 and T2
    EMA_descriptives$completed_T1[idx] <- sum(!is.na(raw_T1[ ,5+v]))/50
    EMA_descriptives$completed_T2[idx] <- sum(!is.na(raw_T2[ ,5+v]))/50
    # (1-Normality) Raw Variables T1 and T2
    if (length(unique(na.omit(raw_T1[ ,5+v]))) > 1) {
            EMA_descriptives$norm_T1[idx] <- 1-shapiro.test(raw_T1[ ,5+v])[[1]] 
    } else  EMA_descriptives$norm_T1[idx] <- 0
    if (length(unique(na.omit(raw_T2[ ,5+v]))) > 1) {
            EMA_descriptives$norm_T2[idx] <- 1-shapiro.test(raw_T2[ ,5+v])[[1]] 
    } else  EMA_descriptives$norm_T2[idx] <- 0
    # (1-Normality) Detrended Variables T1 and T2
    if (length(unique(na.omit(raw_T1[ ,5+v]))) > 1) {
            EMA_descriptives$norm_detrended_T1[idx] <- 1-shapiro.test(detrended_T1[ ,5+v])[[1]]
    } else  EMA_descriptives$norm_detrended_T1[idx] <- 0
    if (length(unique(na.omit(raw_T2[ ,5+v]))) > 1) {
            EMA_descriptives$norm_detrended_T2[idx] <- 1-shapiro.test(detrended_T2[ ,5+v])[[1]]
    } else  EMA_descriptives$norm_detrended_T2[idx] <- 0
    # Means and SDs of raw and imputed, T1 and T2
    EMA_descriptives$mean_raw_T1[idx]     <- raw_T1[ ,5+v]     %>% mean(na.rm = TRUE)
    EMA_descriptives$mean_raw_T2[idx]     <- raw_T2[ ,5+v]     %>% mean(na.rm = TRUE)
    EMA_descriptives$sd_raw_T1[idx]       <- raw_T1[ ,5+v]     %>% sd(na.rm = TRUE) 
    EMA_descriptives$sd_raw_T2[idx]       <- raw_T2[ ,5+v]     %>% sd(na.rm = TRUE) 
    EMA_descriptives$mean_imputed_T1[idx] <- imputed_T1[ ,5+v] %>% mean(na.rm = TRUE)
    EMA_descriptives$mean_imputed_T2[idx] <- imputed_T2[ ,5+v] %>% mean(na.rm = TRUE)
    EMA_descriptives$sd_imputed_T1[idx]   <- imputed_T1[ ,5+v] %>% sd(na.rm = TRUE) 
    EMA_descriptives$sd_imputed_T2[idx]   <- imputed_T2[ ,5+v] %>% sd(na.rm = TRUE) 
  }
}

# rank stat raw data
EMA_descriptives$rank_stat <- EMA_descriptives$completed_T1 * 
                              EMA_descriptives$completed_T2 * 
                              EMA_descriptives$norm_T1 * 
                              EMA_descriptives$norm_T2

# rank stat on detrended data
EMA_descriptives$rank_stat_detrended <- EMA_descriptives$completed_T1 * 
                                        EMA_descriptives$completed_T2 * 
                                        EMA_descriptives$norm_detrended_T1 * 
                                        EMA_descriptives$norm_detrended_T2

saveRDS(EMA_descriptives, file = here("Data", "EMA_descriptives.RDS"))

# to peak at average rank score averages in sample:
# EMA_descriptives %>% group_by(emaLabels) %>% summarise( avg = mean(rank_stat_detrended)) %>% View()

# NOTE
# v = 1: first EMA Variable; located at 6th column in data (thus 5+v)
# ShapiroWilk test stat: 0 = perfect normality, 1 = deviation from normality. 
# Because we want to prioritize more normally distributed variables, we use 1-ShapiroWilk.
# As result, a higher rank statistic means better properties.
```
## Network estimation

### Variable selection for idiographic networks

Because we expect high heterogeneity in sample in terms of which items apply to an individual's experience and variables with high negative skew (e.g. most responses being zero) are problematic for model estimation (violates model assumptions and can lead to non-convergence).
We further want to make the variable selection process reproducible by basing the decision process on statistical criteria.
Selecting variables purely based on statistical properties may result in networks that include only highly similar, closely related variables.
Also, as resulting estimates of network stability depend on which variables are included in the network, having vastly different idiographic networks may confound these estimates.
Therefore, want to optimize variable selection in a way that makes idiographic networks somewhat comparable across individuals, while capturing the unique behaviors that are related to individual psychopathology.
We thus took a hybrid approach of variable selection based ontheoretical as well as statistical grounds:

Each idiographic network will include 3 composite variables (sum scores) which are common across participants: Positive Affect, Negative Affect, and Stress Severity.
Further, we will include single-item variable capturing daily functioning.
Per subject, two additional variables will be selected according to their rank on the following scoring algorithm:

Ranking metric = 1-Shapiro-Wilk test statistic T1 \* 1-Shapiro-Wilk test statistic T2 \*prop completed assessments T1 \* proportion completed assessments T2

The Shapiro-Wilk test statistic tests the null hypothesis that a variable is sampled from a normal distribution, ranging from 0 to 1 [@YaziciYolacan2007, @ShapiroWilk1965].
<!-- Note: Normality of variables is not necessary for estimation, but strong deviations from normality (such as strong negative skew or little variance can contribute to model non-convergence, especially in relatively small samples such as here -->

### Network estimation

Idiographic network models will be estimated on an individual basis.
Contemporaneous and temporal (Lag-1) associations will be estimated in form of a Gaussian Graphical Model using the open-source R package *graphicalVAR* [@EpskampEtAl2018b].
<!-- cite R, todo: summarize how GVAR is estimated --> Lag-1 VAR models estimate temporal associations by predicting each variable by all variables in the network at the previous time point, including itself, using multivariate linear regression.
The tuning parameter $/gamma$, controlling the degree of regularization, will be set to 0.5 (default in R).
Should this result in predominantly empty networks, $/gamma$ will be reduced to 0.25 or 0.
All applied variations to $/gamma$ and their effects on the results will be reported.

<!-- Other methods to estimate idiographic network models exist, which each their own benefits. In contrast to multilevel network estimation, which may be preferred for it's ability to estimate indivududal networks while borrowing infomation from the group-level and requires less data points @EpskampEtAl2018b. However, in multilevel modeling, individual networks are restricted to have the same structure across individuals, which is an unlikely assumption in our heterogeneous sample.-->

<!--- recode network variables so that more score is worse (eg recode positive affect and daily functioning to reverse)
-->

```{r NetworkEstimation, message=TRUE, warning=TRUE}

# empty data frame to store path estimates in long format (two rows per participant, 1 for T1, 1 for T2)
networks <- data.frame("ID" = rep(1:N, each=2),
                      "timeperiod" = rep(c("T1", "T2"), N),
                      # 3:17 Contemporaneous network: lower triangle in col by col format
                      "PCC_PA_NA" = NA,
                      "PCC_PA_Stress" = NA,
                      "PCC_PA_Task" = NA,
                      "PCC_PA_Rank1" = NA,
                      "PCC_PA_Rank2" = NA,
                      "PCC_NA_Stress" = NA,
                      "PCC_NA_tasks" = NA,
                      "PCC_NA_Rank1" = NA,
                      "PCC_NA_Rank2" = NA,
                      "PCC_Stress_tasks" = NA,
                      "PCC_Stress_Rank1" = NA,
                      "PCC_Stress_Ranks2" = NA,
                      "PCC_Tasks_Rank1" = NA,
                      "PCC_Tasks_Rank2" = NA,
                      "PCC_Rank1_Rank2" = NA,
                      # 18:53 Temporal networks: full matrix in col by col format
                      "PDC_PA_PA" = NA,
                      "PDC_PA_NA" = NA,
                      "PDC_PA_Stress" = NA,
                      "PDC_PA_Tasks" = NA,
                      "PDC_PA_Rank1" = NA,
                      "PDC_PA_Rank2" = NA,
                      "PDC_NA_PA" = NA,
                      "PDC_NA_NA" = NA,
                      "PDC_NA_Stress" = NA,
                      "PDC_NA_Tasks" = NA,
                      "PDC_NA_Rank1" = NA,
                      "PDC_NA_Rank2" = NA,
                      "PDC_Stress_PA" = NA,
                      "PDC_Stress_NA" = NA,
                      "PDC_Stress_Stress" = NA,
                      "PDC_Stress_Tasks" = NA,
                      "PDC_Stress_Rank1" = NA,
                      "PDC_Stress_Rank2" = NA,
                      "PDC_Tasks_PA" = NA,
                      "PDC_Tasks_NA" = NA,
                      "PDC_Tasks_Stress" = NA,
                      "PDC_Tasks_Tasks" = NA,
                      "PDC_Tasks_Rank1" = NA,
                      "PDC_Tasks_Rank2" = NA,
                      "PDC_Rank1_PA" = NA,
                      "PDC_Rank1_NA" = NA,
                      "PDC_Rank1_Stress" = NA,
                      "PDC_Rank1_Tasks" = NA,
                      "PDC_Rank1_Rank1" = NA,
                      "PDC_Rank1_Rank2"   = NA,
                      "PDC_Rank2_PA" = NA,
                      "PDC_Rank2_NA" = NA,
                      "PDC_Rank2_Stress" = NA,
                      "PDC_Rank2_Tasks" = NA,
                      "PDC_Rank2_Rank1" = NA,
                      "PDC_Rank2_Rank2" = NA
                      )

# index columns for contemporaneous network (PPC: 3:17) and temporal networks (PDC 18:53) in Networks df
idx_PCC <- 3:17
idx_PDC <- 18:53
# data frame for main outcome statistics
comparisons <- data.frame(ID = 1:N)

# Specification a la BeckJohnson2020, gamma = 0, Lambda range 0.025 to 1
for (p in 1:N){
  # filter data by participant
  d_T1 <- filter(data, participantID==p, day.response %in% 1:50)
  d_T2 <- filter(data, participantID==p, day.response %in% 51:100)
  #  select two highest ranking DPDS/behavior variables, higher rank stat = better
  selectVar  <- filter(EMA_descriptives, 
                  EMA_descriptives$ID == p, 
                  EMA_descriptives$emaLabels %in% 
                    emaLabels[c(11:23, 31:60)]
                  )
  selectVar <- selectVar[order(selectVar$rank_stat_detrended, decreasing = TRUE), ]
  Rank1       <- selectVar[1, 3]
  Rank2       <- selectVar[2, 3]
  # log name of DPDS Variables
  comparisons$Rank1[p] <- Rank1
  comparisons$Rank2[p] <- Rank2
  # define individualized networks
  d_T1 <- select(d_T1, 
                 day.response, 
                 PosAffect, 
                 NegAffect, 
                 StressSev, 
                 tasks, 
                 all_of(Rank1), 
                 all_of(Rank2)
                 )
  d_T2 <- select(d_T2, 
                 day.response, 
                 PosAffect, 
                 NegAffect, 
                 StressSev, 
                 tasks, 
                 all_of(Rank1), 
                 all_of(Rank2)
                 ) 
  # BeckJohnson2020 used gamma = 0, Lambda 0.025:1 by 0.025
  netw_T1 <- graphicalVAR(d_T1, 
                          beepvar = "day.response",
                          gamma = 0,
                          lambda_beta = seq(.025, 1, .0125)
                          )
  cat(paste("T1 participant",p ,"converged"))
  netw_T2 <- graphicalVAR(d_T2, 
                          beepvar = "day.response",
                          gamma = 0,
                          lambda_beta = seq(.025, 1, .0125)
                          )
  cat(paste("T2 participant",p ,"converged"))
  
  # store full network objects in global environment for later inspection
  assign(paste0("ID",p,"_T1"), netw_T1)
  assign(paste0("ID",p,"_T2"), netw_T2)
  # save network objects for detailed inspection / plotting
  saveRDS(netw_T1, here("data", "network_objects", paste0("ID",p,"_T1.RDS")))
  # extract path estimates for easy comparison
  networks[networks$ID == p, idx_PCC] <-
    rbind(netw_T1$PCC[lower.tri(netw_T1$PCC)] %>%
            as.numeric()%>%
            as.vector() %>%
            round(digits=4),
          netw_T2$PCC[lower.tri(netw_T2$PCC)] %>%
            as.numeric()%>%
            as.vector() %>%
            round(digits=4)
          )
  networks[networks$ID == p, idx_PDC] <- 
    rbind(netw_T1$PDC %>%
            as.numeric()%>%
            as.vector()%>%
            round(digits = 4),
          netw_T2$PDC %>%
            as.numeric()%>%
            as.vector()%>%
            round(digits = 4)
          )
}

saveRDS(networks, file = here("data", "networks.RDS"))

networks$nr_zero_PCC        <- sum(networks[ ,idx_PCC] == 0)
networks$nr_zero_PDC        <- sum(networks[ ,idx_PDC] == 0)
networks$nr_estimated_PCC   <- sum(networks[ ,idx_PDC] != 0)
networks$nr_estimated_PDC   <- sum(networks[ ,idx_PDC] != 0)
networks$pr_zero_PCC        <- rowMeans(networks[ ,idx_PCC] == 0)
networks$pr_zero_PDC        <- rowMeans(networks[ ,idx_PDC] == 0)
networks$nr_pos_PCC         <- sum(networks[ ,idx_PCC] > 0)
networks$nr_pos_PDC         <- sum(networks[ ,idx_PDC] > 0)
networks$nr_neg_PCC         <- sum(networks[ ,idx_PCC] < 0)
networks$nr_neg_PDC         <- sum(networks[ ,idx_PDC] < 0)
networks$connectivity_PCC   <- round( networks$nr_estimated_PCC /length(idx_PCC) * 100, 1)
networks$connectivity_PDC   <- round( networks$nr_estimated_PDC /length(idx_PDC) * 100, 1)
  
sum(networks$pr_empty_PCC == 1) # 1 empty contemp net total
sum(networks$pr_empty_PDC == 1) # 45 empty temp net total

for (p in 1:N) {
  # dynamic index for T1 and T2 rows per person in 'networks' data frame
  idx_p_T1  <- (p-1)*2+p
  idx_p_T2  <- (p-1)*2+p+1
  PCC_T1    <- networks[idx_p_T1, idx_PCC] %>% as.numeric()
  PCC_T2    <- networks[idx_p_T2, idx_PCC] %>% as.numeric()
  PDC_T1    <- networks[idx_p_T1, idx_PDC] %>% as.numeric()
  PDC_T2    <- networks[idx_p_T2, idx_PDC] %>% as.numeric()
  # correlation of edge weights
  comparisons$PCCs_cor[p] <- cor(PCC_T1, PCC_T2, method = "spearman")  
  comparisons$PDCs_cor[p] <- cor(PDC_T1, PDC_T2, method = "spearman") 
  # rank order correlations of edge weights ("Spearman Rank Correlatins" FriedEtAl2018)
  comparisons$PCCs_rank_cor[p] <- cor(rank(PCC_T1), rank(PCC_T2),method = "spearman")
  comparisons$PDCs_rank_cor[p] <- cor(rank(PDC_T1), rank(PDC_T2),method = "spearman")
  # Jaccard similarity, https://en.wikipedia.org/wiki/Jaccard_index#Similarity_of_asymmetric_binary_attribute
  # proportion of shared non-zero edges relative to number of edges which are non-zero in either, but not both, networks 
  comparisons$PCCs_Jaccard <-   sum(PCC_T1!=0 & PCC_T2!=0) / sum(PCC_T1!=0 | PCC_T2!=0)
  comparisons$PDCs_Jaccard <-   sum(PDC_T1!=0 & PDC_T2!=0) / sum(PDC_T1!=0 | PDC_T2!=0)
  # proportion empty edges both netw
  comparisons$PCCs_prop_empty     <- mean( PCC_T1 == 0 & PCC_T2 == 0)
  comparisons$PDCs_prop_empty     <- mean( PDC_T1 == 0 & PDC_T2 == 0)
  # proportion of edges with equal sign or both zeroin network
  comparisons$PCCs_prop_equ_sign  <- mean( (PCC_T1 == 0 & PCC_T2 == 0) |
                                           (PCC_T1 > 0 & PCC_T2 > 0) |
                                           (PCC_T1 < 0 & PCC_T2 < 0))
  comparisons$PDCs_prop_equ_sign  <- mean( (PDC_T1 == 0 & PDC_T2 == 0) |
                                           (PDC_T1 > 0 & PDC_T2 > 0) |
                                           (PDC_T1 < 0 & PDC_T2 < 0))
}

hist(comparisons$PCCs_cor, breaks = N)
hist(comparisons$PDCs_cor, breaks = N)
# checl sascha False Alarm code https://osf.io/xh87b/
# check sForbes Quantifying code https://osf.io/qcrjk/

# mean mode median, sd
# rank roder correlation: rank edgeweights and correlate their ranks with spearman
####################
# extract metrics per network
# density
# proportion empty edges

# comparisons
# correlation adj matr PCC
# correlation adj matr PDC
# 
# prop of edges that are estimated as empty, positive or negative in both networks =
# jacqard similarity: (count of agreeing categories)/ (count of all edges)
# 
####################
# save network objects (for later), here("data",)
# assign(paste0("ID",p,"_T1"), netw_T1) #4.4mb per netw
# save RDS in subfolder using here()

# TODO build in option for empty networks and non-converging networks

# kappa matrix  is partial contemp corr PCC
# beta matrix is partical directed correlations PDC
# 
# compute comparison metric, see Mansueto code for examples
# vechs(cont) for contemp netw

# TODO plot networks
# Names for the Plot :
names <- c("PA", "NA", "Stress", "Tasks", toupper(Rank1), toupper(Rank2) )

# for plots, fix layout and node strength for comparability
# qgraph(netw_T1$PCC,  layout = "circle", nonsig = "hide", theme = "colorblind", title = "Contemporaneous", labels = names, vsize = 10,  asize = 10, mar = c(5,5,5,5)) 

```

## RQ1: Network comparisons

As an index of temporal stability, we compare idiographic networks estimated for the first and last 50 days of measurement by correlating estimated network edge weights.
Other conceptualizations and tests of network similarity have been proposed, the most popular one being the Network Comparison Test (NCT, @vanBorkuloEtAl2017 ).
<!-- Do we need Fishers R to Z transformation to transform correlations to normal distr for the regression? Beck does it in her Covid paper -->
<!-- when comparing the matrices, need to remove diagnonal!!! for contemporaneous netwokrs -->
TODO: explain comparison metrics used and their meaning
TODO: briefly mention Frumkin discussion for context


## RQ2: LASSO predictive regression

The following baseline variables will be included tested as predictors: - Sex - Age - Income - past six months: Happy - past six months: Mobility - past six months: Impulse - past six months: Relationships - past six months: Work - past week: Suicidality - past year: Operation - Handicap - Cigarette - Alcohol - Substance - Time since last psychological treatment (including "never") - Treatment provider (as ordinal scale) - Comorbid / previously diagnosed depression - Comorbid / previously diagnosed Anxiety - Comorbid / previously diagnosed Substance abuse or other addiction (merge level 2 and 3, see codebook) - Comorbid / previously diagnosed Schizophrenia - Comorbid / previously diagnosed Eating Disorder - Relationship / Family problems - Life Satisfaction (Mean, Satisfaction with Life Scale) - Neuroticism (NEO-FFI) - Extraversion (NEO-FFI) - Openness (NEO-FFI) - Agreeableness (NEO-FFI) - Conscientiousness (NEO-FFI) The following statistical aspects will be included as predictors: - Total number of imputed data points per individual: Due to the imputation process, higher proportions of missing data may inflate estimates of temporal stability
<!-- perhaps include means and variances of items as well, but difficult to code.-->

To test which person-specific attributes or statistical factors predict temporal network stability, we use predictive regression with least absolute shrinkage and selection operator (LASSO, @Tibshirani1996, @McNeish2015 ) regularization.
In predictive regression, a model is optimized for it's ability to predict the outcome variable in a novel sample [@WestfallYarkoni2016].
This is in contrast to the more commonly used explanatory regression, in which model parameters are optimized to explain maximum variance in the observed sample, which can lead to overfitting and poor out-of sample utility.
LASSO regularization shrinks small beta coefficients to zero, effectively excluding less relevant predictors from the model.
The amount of shrinkage applied, determined by the tuning parameter lambda, is optimized for predictive accuracy using K-fold cross-validation.
Using LASSO regularization, we can explore a wide range of potential predictors while selecting a parsimonious model without relying om arbitrary significance cut-offs or prior theory for variable selection.
Data will be spit at random into a training set (80% of cases) and test set (20% of cases), fixing the random number generator at 1821 for reproducibility of the analysis.
The training set will be used for model estimation using K-fold cross validation with 5 folds.
Missing data (e.g. due to network model non-convergence or missing baseline variables) will be handled using listwise deletion.
The model with least prediction error during cross validation will be tested for out-of sample predictive accuracy in the test set.
If predictive accuracy is satisfactory (root mean squared error $<$.20, meaning predicted correlation coefficients are within +/- .10 of original correlation coefficient estimates 
<!--arbitrary value, but seemed reasonable: the ROT classifies degrees of correlation strength in steps of 20, so a deviation of 10 would lead to a similar interpretation in these terms)-->, the resulting model will be fit on the complete data set to extract unbiased parameter estimates and estimated variance explained.
<!-- feedback eiko: no threshold for interpreting model but do mention r^2 and out of sample pred acc-->

# Results

## RQ1
- Brief summary of estimation process: with which parameters did model converge or not? provide estimates with different gamma and lamba in Appendix, together with histograms of outcome variables (nr of empty networks, failed convergions, and plots of outcome measures)
- for evaluated networks (beckjackson specs): plot cumulative distr of correlations, or histogram, and perhaps a heatmap of the estimated path coefficients ( per person, T1 value of path on X, T2 value on Y, separately for contemp and temporal networks. So here we do not know which path it is, every person has bunch of path coefficients in heatmap)
- I could also do 1 heatmap / scatterplot per person for the two separate plots
- demonstrate 2 or 4 cases (eg. highly stable, highly instable, mostly empty, very dense network, and relate it to the metrics. explain what the metrics mean.
- Also, for those 2 px, provide mean and sd of all items in Table for both time points
- ! when plotting, impose equal layout and node strength"maximum value of node strength", "identical layouts Fruchterman Reingold algorithm-->
- visualize all included variables (not rank1 and rank 2) as spaghettiplot per individual across the 100 days. https://www.r-graph-gallery.com/parallel-plot.html

## RQ2

## Example participants
- demonstrate 2 or 4 cases (eg. highly stable, highly instable, mostly empty, very dense network, and relate it to the metrics. explain what the metrics mean. And include relevant demographics of contributing factors.
- Also, for those 2 px, provide mean and sd of all items in Table for both time points
- ! when plotting, impose equal layout and node strength"maximum value of node strength", "identical layouts Fruchterman Reingold algorithm-->
- visualize all included variables (not rank1 and rank 2) as spaghettiplot per individual across the 100 days. https://www.r-graph-gallery.com/parallel-plot.html

# Discussion

## Strengths
-> implications *assuming that change scores indeed reflect change in data generating process*
-- assuming that lack of stability is due to instability of data generating process: challenge to idiographic modelling, stationarity assumption and power problems, potential solutions are being developed and needed (multilevel estimation, baysian, timevarying var)
<!-- mention time varying auto regressive network models @HaslbeckBringmanWaldorp2020 -->

-- assuming that stability is because process is stable: we probably don't need to worry about losing power by expanding time window over in this case 100 dates
-- intra-individual variation: changes in network structure may be related to factors related to the individual (eg traits, circumstances, quality of the data, response style... provide list)

## Limitations
### Exploratory: 
-   limitations of this data set for this research q (e.g. sample size / power)
-   constraints to generalizability (population, designs, time frame, estimation method)
-   Kalman imputation assumes MCAR, but likely there is some bias.
-   We focus on similarity of global network structure, but there are many more ways to descibe and interpret networks which may or may not be relevant for replicability: network comparison test, predictive networks models, sensitivity and specificity of recovered edges if true network (assumed to be) known

### Interpretation:
- what can we conclude from low stability values?
-- cause change in estimates not known: not necessarily instability of process, but possibly also inaccurate estimates and overfitting possible issues (high regularization applied due to low power)
-- high stability: possibly also a chance finding (parameter accuracy) 

## Future directions
### Stationarity
### Detrending
### Temporal Stability
-   relate to idea of monitoring change, critical slowing down, phase transitions, ROM
### Parameter accuracy: (eg., a regression coefficiants t-test in a linear model)
-   relate to stationarity assumption
### Measurement error
-   Important assumption made by (idiographic) network models is that constructs were measured without error. Little published research on this, but eg. SchreuderEtAl2020 assessed participants interpretation of EMA items over the course of 6 months and concluded interpretation was consistent. Changes in item interpretation also known as measurement invariance or response shift bias.
### stationarity and temporal stability as features / traits of an individual?
-   is extending measurement period instead of increasing frequency a good solution, eg.
    with planned missingness, or is non-stationarity and low replicability too much of concern?
    
# Conclusion
-   warrants caution regarding the inferences we draw (momentary impression of item correlations in certain period of time vs. stable process which extends beyond this period and could inform interventions (TODO: related to predictive value of networks?)

\newpage

# References


\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

{\#refs custom-style="Bibliography"}

\endgroup
