---
title             : "Within-person replicability of idiographic networks"
shorttitle        : ""

author: 
  - name          : "Ricarda K. K. Proppert"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : ""
    email         : "ricarda.proppert@gmail.com"

affiliation:
  - id            : "1"
    institution   : "Clinical Psychology, Leiden University, The Netherlands"


abstract: |
  Evidence-based mental health programs have long conceptualized mental disorders in terms of interactions between thoughts, feelings, behaviours and external factors.
  
  Idiographic network models are a relatively novel way of modeling such intra-individual psychological processes. These methods are not without limitations, and concerns have been raised about the stability and accuracy of estimated networks.
  
  While methods to assess network parameter accuracy have been developed for cross-sectional data, no such method exists for single-subject data. The extend to which idiographic networks are stable, or vary over time, is unknown.
  
  In the current work, we reanalyse daily symptom records of people with personality disorders to explore the stability of idiographic networks over time, as well as the degree to which network stability varies across individuals. We further explore antecents that may relate to inter-individual variation in network stability using predictive LASSO regression.

  
keywords          : ""
wordcount         : ""


floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_doc

bibliography: references.bib
---

```{r setup, include = FALSE}
# install.packages("here")
setwd("~/GitHub/thesis_repo")
library(here)
here::i_am("ThesisAnalysisV1.Rmd") # find directory where file is in

# install.packages("tidyverse")
library("tidyverse")

# install.packages("papaja")
library("papaja")

# install.packages("imputeTS")
library("imputeTS")

# install.packages("graphicalVAR")
library("graphicalVAR")

# install.packages("qgraph")
library("qgraph")

# install.packages("OpenMX")
library("OpenMx")

# r_refs("r-references.bib")


# Notation: 
# ---------
#     data:   full data set used. 
#             In chunks that manipulate the whole data set (eg., imputation, detrending) I reassign the manipulated data object to 'data', so that results can easily be rerun on not detrended / not imputed data by simply not evaluating the previous code chunk
#     p:      participant (itiration in loop)
#     v:      variable (itiration in loop)
#     d:      temporary data object in loop


# sessionInfo()
# -------------
# R version 4.0.3 (2020-10-10)
# Platform: x86_64-apple-darwin17.0 (64-bit)
# Running under: macOS Big Sur 10.16
```

```{r readData}
# read data
data <- read.csv(here("data", "dd100-proppert.csv"))

# data is in long format
N <- data$participantID %>% unique()
range(N) # 1 : 116
length(unique(N)) # 112
setdiff(1:116, N) # Participants  8, 25, 45, 92  had been excluded by original authors because more than 30% of responses were missing

# rename IDs for easier looping
data$participantID <- factor(data$participantID, 
         levels = unique(data$participantID), 
         labels = 1:length(unique(data$participantID))) %>% 
  as.numeric()
N <- data$participantID %>% unique() %>% max()
```


<!-- TODO: 
-    add Eiko as Supervisor
- place authornote in YAML again, currently throws error due to papaja bug. 
Issue: https://github.com/crsh/papaja/issues/225

authornote: "Master thesis for the Research Master Clinical- and Health psychology (combined track) under supervision of Dr. Eiko Fried. We thank Dr. Aidan Wright, Pittsburg University, and his colleagues for sharing their data with us. The reproducible analysis script can be downloaded at "link Github", data are available upon request."

- Abstract needs Results and Implication
- Specify authorship roles in YAML (see Master guidelines)
- wording consistent: stability vs accuracy
-->```
# Introduction

## Idiographic psychological networks

Idiographic network models are of growing interest to clinical psychology because they may address two recently voiced calls in clinical psychology: Firstly, there seems to be a need for psychological research to re-orient towards idiographic methods that study intra-individual processes as opposed to group-level differences [@Molenaar2004].
<!--TODO mention that results from grouplevel may not generalize to individual level, eg. @Bulteel2018 --> Secondly, scholars are proposing a paradigm shift away from reductionism towards studying the complexity of psychological phenomena.
The Network theory of mental disorders [@BorsboomCramer2013; @CramerEtAl2010] attempts to integrate psychology with insights and methods from complexity science, proposing a novel but well-received theoretical framework to study and understand the underpinnings of psychopathology.
The network theory of mental disorders conceptualizes psychopathology as an emergent state of dynamically interacting psychological symptoms and states, as well as factors external to this system.
<!-- Eiko: can also be somatic sx, but R can't find way to keep sentence readible--> This account seems closely aligned with established clinical practices where informal case conceptualizations in form of path diagrams are used to describe the proposed mechanisms of a given disorder (@BurgerEtAl2020, @ScholtenEtAl2020).
<!--For example, dysfunctional cycle of mutually interacting stressors, thoughts, feelings and behaviors.-->

With the ongoing development of statistical methods to visualize and quantify such system's structures and behaviors, called psychological networks [@EpskampEtAl2018], an increasing body of research is now applying this framework to study symptom networks in people with mental disorders.
<!-- ref Sascha Diss, and perhaps mention / explain "psychonetrics"--> Psychological networks consist of elements (nodes) and their pairwise interactions (edges), together forming a complex system.
Nodes represent psychological or other variables, such as psychological or somatic symptoms, stressors, or behaviors.
Edges represent pairwise relationships between these variables.
These relationships may be directed or undirected, positive or negative, and can differ in strength.
These associations and are thought to result from of a multitude of mechanisms which may not necessarily be known or defined.


<!-- talk about stationarity assumption and time-varying autoregressive models eg HaslbeckBringmanWaldop2018, 
and https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7186258/ -->

## Temporal stability of networks

Most of the early psychological network research focused on group-level analyses and cross-sectional comparisons, mainly for practical reasons such as availability of existing data-sets.
Few studies have investigated longitudinal network stability at a group level (e.g., in veterans pre- to post-combat, @SegalEtAl2020; military veterans, @vonStockertEtAl2018; among adolescent earthquake survivors, @GeEtAl2019; and in patients with Anxiety and Depression, @CurtissEtAl ).

Besides group-level analyses, there is growing interest in idiographic network models of intra-individual symptom dynamics.
Researchers in clinical psychology in particular hope that such models could resolve what is known as the Therapist's dilemma <!-- explain and ref --> (eg., @FrumkinEtAl2020, @HoweEtAl2020, @CavigliaColeman2016, @HoffartJohnson2020).
In clinical psychology research, not only momentary network structures are of interest, but especially *changes* in network structure over time.
For example, changes in network structure may indicate therapeutic progress [@ThononEtAl2020] or relapse [@WichersEtAl2016].
Even subtle changes in network structure are of interest, as they are thought to act as potential early warning signals which may predict future major change, i.e. a system's phase transition from a healthy attractor state to a disordered one.
For example, signs of critical slowing down, showing as increased auto-correlations and variances of items, have been demonstrated to signal a patient's relapse into depression upon stopping antidepressant treatment (@WichersEtAl2016; for similar work on resilience, see @KuranovaEtAl2020).

<!-- mention case formulation paper burger 2020 -->

<!-- feasibility study frumkin -->

<!-- Stability and accuracy of psychological networks TODO: explain vocab as used in this work. Accuracy = accuracy of parameter estimates Stability = stability of underlying process -->

<!--In practice, these two are rarely distinguished. A major reason for this is that methods to estimate network parameter accuracy have only recently become available and implemented in popular software packages (estimating stability paper) Furthermore, for the most popular estimation method graphical Var / ml Var (vector autoregressive models) they are not available yet. -->

Empirical work is published in which differences in network structures are interpreted at face value.
For example, @ThononEtAl2020 followed three psychiatric patients over the course of treatment and interpreted changes in idiographic network structures over time as additional evidence for and a description of the observed therapeutic change.
Such a substantial interpretation of network instability hinges on the assumption that differences in estimated network parameters accurately reflect change in the data-generating mechanism.
This assumption rests on two conditions: First, parameter estimates need to be an accurate reflection of the true underlying relationship.
Second, the underlying mechanism would have remained stable if no intervention took place (related to the assumption of stationarity made during model estimation).
Neither of these conditions can currently be evaluated in rigorous ways for idiographic network models.
Methods to assess the accuracy of parameter estimates have been developed for group-level estimation procedures [@EpskampEtAl2018], but are not yet available for idiographic estimation methods.
<!-- explain estimating accuracy methods and why not available for gVAR and idiographic models --> The degree to which underlying dynamic psychological processes are stable within individuals over time is unclear.
Current literature investigating the temporal stability of idiographic networks mostly focuses on settings where change is expected to occur, e.g., in response to psychological treatment [@ThononEtAl2020], discontinuation of antidepressant medication [@WichersEtAl2016], or the COVID-19 pandemic [@BeckJackson2021].
To our knowledge, only one study has investigated the temporal stability of psychological networks in situations where no change would be expected.
@BeckJackson2020a investigated the consistency of idiographic personality over the course of two years.
They reported high consistency among contemporaneous associations, low consistency of temporal associations, and considerable interpersonal variability in the stability of networks.

<!-- BeckJackson2020 use GIMME and no detrending, super helpful prereg and code on osf + gh -->

`
<!--
## Current challenges in research on idiographic networks

Explain estimation procedures and assumptions in more detail: - explain graphicalVAR estimation - stationarity - multivariate normality - no measurement error - mention related methods that we do not use here, and why (mlVAR, baysian, all the other not so networky idioraphic models (see idio review)

-   require high power, which is difficult to obtain in longitudinal N=1 study (burdensome)

-   even if power is high and networks can be estimated reliably, longitudinal stability of networks is unknown (ie how they generalize to different time within same individual)

-   stability of estimates is an important issue (beyond scopoe here but explain this as important limitation)

-   even if estimates stable, replicability not warranted:

-   How stable is the data generating process we try to capture? Is this the process of interest? Is it malleable? Phase transitions?

-   feasibility
-->

<!-- idea eiko: "There are 5 main challenges, bla bla bla. In this work, we will tacke 1,2  and 3". Rest goes into limitations.

"where ema res is by nature confined to assess snapshots we dont now how well networks replicate within a person even
"
q of replicability "empirical replicability"  vs statistical replicability (simulation studies give good evidence) - see forbes

STATIONARITY: Eiko says it's univariate, so (means and variances, so 1st and 2nd order only)

### Text Box: List of most relevant factors influencing "snapshottiness" of idio networks
List statistical and substantial reasons
response shift bias
measurement error
stationartiy
§change"
motivation
response time? as potential covariate?

statistical things:
- sparsity (
- density: strength of sum of edges
- sparsity:  "percentage non-zero cells" 
-->

## Aim of this study

<!-- A recent simulation study (preprint Mansueto), ... required power would be bla.-->

The present study aims to assess the stability of idiographic networks of psychopathology, as well as exploring potential person-specific factors and statistical factors predicting interpersonal variation in temporal network stability.

We will address the following research questions:

RQ1: How stable are estimated idiographic network structures over time?

RQ2: What factors, person-specific or of statistical nature, predict inter-individual variation in intra-individual replicability?

To this end, we re-analyze daily diary data collected in a clinical sample of people diagnosed with a personality disorder [@WrightEtAl2015a; @WrightSimms; @WrightEtAl2015].
Participants (N=116) provided once-daily ratings of their mood, behaviors, and daily stressors over the course of 100 consecutive days.
To assess intra-individual stability of idiographic networks, we will fit separate idiographic networks on the first and last 50 days of measurement.
Network structures of each individual's first vs. last 50 days of measurements will be compared by correlating estimated path estimates, with high correlation indicating high temporal stability.
Finally, person-specific antecedents and statistical factors will be tested for their ability to predict interindividual variation in network stability using predictive LASSO regression.

## Data Set

A summarized version of the data are published HERE, an overview of all available variables can be found in Appendix A.

The original study design by which the data were collected are described in detail in previous publications [@WrightEtAl2015; @WrightSimms; @WrightEtAl2015a].
The study investigated daily dynamics in affect, stress, andexpressions of personality disorder, along with some lifestyle variables such as self-reported sleep, drug and alcohol use, and overall functioning.
Participants were recruited from an ongoing clinical study (N=628, @SimmsEtAl2011) that targeted individuals who had received psychiatric treatment within the previous two years, recruited via flyers distributed at mental health clinics across Western New York, USA. They were invited for study participation if they met the diagnostic requirements of any personality disorder during the initial structured clinical interview conducted for the parent study (SCID-II, TODO REF), and if they had daily internet access via a computer or mobile device.
116 participants were initially enrolled, of which 101 participants completed at least 30 of the desired 100 daily measurements.

Participants completed daily measurements over the course of 100 consecutive days.
Participants completed surveys at roughly the same time each night, depending on their individual schedule.
Therefore, therefore, this data set is expected to meet the assumption of roughly equal time intervals in between measurements [@Epskamp2020].
Furthermore, the original authors reported stability of means and variances of expressions of daily psychopathology in @WrightSimms, indicating that the assumption of stationarity may be realistic [@Epskamp2020].
Participants started measurements asynchronically, so that no sample-wide history effects should affect our results on longitudinal within-person stability of estimated networks.

<!-- We chose this data set for this study for three main reasons: It met our minimum requirement of spanning a sufficiently long time scale of intensive longitudinal data, it was readily available, and we believe the design to be a good representation of a feasible clinical setting: Daily records are an already well-established method for enhancong diagnosis and monitoring patients daily lives, and asks less burden of participants compared to more intensive EMA methods. Secondly, the duration of 100-days consistent data collection was long enough so that splitting the data seemed feasible. Recorded variables - although being collected in a sample of people diagnosed with a personality disorder - were general enough to not be restricted to a certain mental disorder. -->

<!--We believe this data set to be representative of a clinically realistic sampling scheme for two reasons: Firstly, measuring symptoms daily as opposed to more often (eg. 4 times daily, as often used in ESM), is an already established method to enhance clinical diagnosis of certain disorders (find info), and even used as therapeutic technique in evidence-based clinical treatments (tracking mood to better understand ones emotional life, find source?). Secondly, daily measurements with a timescale of 1 day in between measuements may be desirable for modeling variables that typically fluctuate on a daily level only and are more within the control of the patient, such as sleep or exercise. Furthermore, the time frame of 100 days may strike a balance of being at the lower end of what may be needed for sufficient statistical power, and at the higher end of what may be an acceptable burden to pose on clinically distressed patients, as well as realistic for typical treatment durations.-->

<!-- Representativeness of realistic clinical designs: Applications of idiographic network models are currently scarce, mainly because they require well-powered data that necessarily come at increased burden to the participants. To improve power in single-subject longitudinal designs, such as ESM / EMA or daily diary studies, Participants are required to either provide more intensive momentary data, or provide less intensive (eg., daily) records over a longer period. Specific power requirements for idiographic networks are still unknown. Some simulation work suggests that ..... , depending on the number of estimated nodes and amount of missingness. Given that daily diaries a more established method of data collection in clinical research and mental health care, and assuming that it poses less burden on participants compared to more intensive sampling frames, we hope that this data is at the higher end of what seems realistic and feasible for future study designs and applications. Simultaneously, although no precise power estimates are currently available, we hope that meets the minimum power needed for our analyses (up to 50 data points per individual, per network). -->

## Analysis plan

### Data pre-processing:

1)  Missing data:

Participants with more than 30 days of non-response, or more than 15 in either the first or last 50 days of measurement, will be excluded from the analyses.
Remaining missing data points of daily diary variables will be imputed using the Kalman Filter [@Harvey1989] , which has been shown to perform well in simulation studies [@MansuetoEtAl2020].

```{r exludeMissing}
# count nr of missing per px
idx_missing <- matrix(NA, N, 7)
colnames(idx_missing) <- c("ID", "CompletedTotal", "CompletedT1", "CompletedT2", "Include", "Var_Tasks_T1", "Var_Tasks_T2")

for (p in 1:N){
  # ID
  idx_missing[p,1] <- p
  d <- filter(data, data$participantID == p)
  # CompletedTotal    
  idx_missing[p,2] <-  unique(d$total.days)
  d1 <-  filter(d, day.response %in% 1:50, missingResponse == 1)
  # CompletedT1      
  idx_missing[p,3] <- dim(d1)[1]
  d2 <-  filter(d, day.response %in% 51:100, missingResponse == 1)
  # CompletedT2      
  idx_missing[p,4] <- dim(d2)[1]
  #check var in tasks
  d3 <- filter(d, day.response %in% 1:50)
  idx_missing[p,6] <- var(d3$tasks, na.rm = T) > 0
  d4 <- filter(d, day.response %in% 51:100)
  idx_missing[p,7] <- var(d4$tasks, na.rm = T) > 0
  # Include; TRUE if more than 70 responses overall, and more than 35 both T1 and T2
  # ALSO EXCLUDING 6 PARTICIPANTS WITH ZERO VAR IN TASKS
  idx_missing[p,5] <- idx_missing[p,2]>70 & idx_missing[p,3]>35 & idx_missing[p,4]>35 &       idx_missing[p,6]==T & idx_missing[p,7]==T
}

# included IDs
idx_missing <- idx_missing %>% as.data.frame() %>% filter(Include == TRUE)

# exclude participants
data <- filter(data, participantID %in% idx_missing$ID)

# rename IDs again for future looping
data$participantID <- factor(data$participantID, 
         levels = unique(data$participantID), 
         labels = 1:length(unique(data$participantID))) %>% 
  as.numeric()

# N: participants retained
N <- max(unique(data$participantID)) #87

```

### RQ1: Temporal stability estimation

#### Variable selection

Because we expect high heterogeneity in sample in terms of which items apply to an individual's experience and variables with high negative skew (e.g. most responses being zero) are problematic for model estimation (violates model assumptions and can lead to non-convergence).
We further want to make the variable selection process reproducible by basing the decision process on statistical criteria.
Selecting variables purely based on statistical properties may result in networks that include only highly similar, closely related variables.
Also, as resulting estimates of network stability depend on which variables are included in the network, having vastly different idiographic networks may confound these estimates.
Therefore, want to optimize variable selection in a way that makes idiographic networks somewhat comparable across individuals, while capturing the unique behaviors that are related to individual psychopathology.
We thus took a hybrid approach of variable selection based ontheoretical as well as statistical grounds:

Each idiographic network will include 3 composite variables (sum scores) which are common across participants: Positive Affect, Negative Affect, and Stress Severity.
Further, we will include single-item variable capturing daily functioning.
Per subject, two additional variables will be selected according to their rank on the following scoring algorithm:

Ranking metric = 1-Shapiro-Wilk test statistic T1 \* 1-Shapiro-Wilk test statistic T2 \*prop completed assessments T1 \* proportion completed assessments T2

The Shapiro-Wilk test statistic tests the null hypothesis that a variable is sampled from a normal distribution, ranging from 0 to 1 [@YaziciYolacan2007, @ShapiroWilk1965].
<!-- Note: Normality of variables is not necessary for estimation, but strong deviations from normality (such as strong negative skew or little variance can contribute to model non-convergence, especially in relatively small samples such as here -->



```{r logLoops}
emaLabels <- labels(data[, 6:75])[[2]] # store labels of EMA variables

# empty dataa frame to log loop
log_loops <- data.frame(
  ID = rep(c(1:N), each = length(emaLabels)), 
  emaVar = rep(seq_along(emaLabels), N),
  emaLabels = rep(emaLabels, N)
  ) 
  
```

```{r varSelection1}
# v = 1: first EMA Variable; data[ ,1:5] are columns in data before 1st EMA variable, thus we use 5+v in all loops itirating on data
# ShapiroWilk test stat: 0 = perfect normality, 1 = deviation from normality. 
# Because we want to prioritize more normally distributed variables, we use 1-ShapiroWilk.
# As result, a higher rank statistic means better properties
 
# Proportion completed T1
log_loops$completed_T1 <- numeric( N * length(emaLabels) )
for (p in 1:N){
    d <- filter(data, participantID==p, day.response %in% 1:50)
    for ( v in seq_along(emaLabels)){ 
     log_loops$completed_T1[(p-1)*length(emaLabels)+v] <- sum(!is.na(d[ ,5+v]))/50 # proportion completed
    }
  }

# Proportion completed T2
log_loops$completed_T2 <- numeric( N * length(emaLabels) )
for (p in 1:N){
    d <- filter(data, participantID==p, day.response %in% 51:100)
    for ( v in seq_along(emaLabels)){
     log_loops$completed_T2[(p-1)*length(emaLabels)+v] <- sum(!is.na(d [ ,5+v]))/50 # proportion completed
    }
  }

```

#### Missing Data Imputation

```{r Kalman, eval= TRUE}
#   apply Kalman filter to impute at level of participant, based on full 100d
data_imputed <- data

library("imputeTS")

for (p in 1:N){
    for (v in seq_along(emaLabels)){
      if (length(unique(na.omit(data[data_imputed$participantID == p, 5+v]))) > 1){
        data_imputed[data_imputed$participantID == p, 5+v] <- na_kalman(data[data_imputed$participantID == p, 5+v]) # throws error, see Note below
        cat("<- T ","ID",p,"var",v, sep = "")         # for check
      } else  data_imputed[data_imputed$participantID == p, 5+v] <- unique(na.omit(data[data_imputed$participantID == p, 5+v]))
        cat("<- F","ID",p,"var",v, sep = "")          # for check
    }
}

# glimpse(data_imputed)
# glimpse(data)
data <- data_imputed 

# NOTE:
# error is thrown when assigning imputed data to df:
# 
# possible convergence problem: 'optim' gave code = 52 
# and message ‘ERROR: ABNORMAL_TERMINATION_IN_LNSRCH’            [,1]      [,2]
# 
# I checked the iterations with errors manually, the imputation and assignment seem to work despite error, so I continue.
# 
# example of itiration w errors: 
# i = 86; v = 41
# cbind( data_imputed[data_imputed$participantID == p, 5+v] , na_kalman(data[data_imputed$participantID == p, 5+v]))
```

#### Detrending linear effects

Variables used for network estimation will be linearly detrended across the 100 day time course, at the level of the individual.
<!-- chose individual bec people are not measured at the same time --> <!-- no detrending for 2 reasons: a) not many study report doing it, the two studies most closely related to mine don't either (Beck Github), and b) I don't think I / we understand how it relates to stationarity vs change in network structure, so I rather not either detrend different time effects in T1 vs T2, vs detrend across T1 and T2 and potentially inflate network similarity bec. why would I in one instance assume the true process is stationary, and in another investigate whether it isn't? HOWEVER Wichers2016 critical transition paper DID detrend and apparently it didnt affect results... mhh. CODE: see email eiko, in dropbox, no attachment-->

```{r detrending, eval = T}
# We detrend at level of participant, using full 100d, and decided to detrend regardless of significance level of linear trend

data_detrended <- data # copy of data to detrend

for (p in 1:N) {
  for (v in seq_along(emaLabels)){
    formula <- as.formula(paste0(emaLabels[v], " ~ 1 + day.response")) # define formula for linear trend
      trend_v <- lm(formula, data = data[data$participantID == p, ]) # fit model per person per var
      data_detrended[data_detrended$participantID == p, 5+v] <- residuals(trend_v) # save detrended var
  }
}; rm(p, v, formula, trend_v)


data <- data_detrended



# NOTE: 
# we for now decided to detrend irregardless of significance of linear effect. # To check for significance of alpha, include below code in loop: 

# alpha         <- 0.05 # define alpha for linear trends
# check alpha level and change if not sign ()
      # if (anova(trend_v)[["Pr(>F)"]][1] < alpha) {
      # } else ...
```



```{r varSelection2}
# (1-Normality) t1, 
log_loops$norm_T1 <- numeric( N * length(emaLabels) )
for (p in 1:N){
    d <- filter(data, participantID==p, day.response %in% 1:50)
    for (v in seq_along(emaLabels)){
      if (length(unique(na.omit(d[ ,5+v]))) > 1) {
           log_loops$norm_T1[(p-1)*length(emaLabels)+v] <- 1-shapiro.test(d[ ,5+v])[[1]]
      } else
           log_loops$norm_T1[(p-1)*length(emaLabels)+v] <- 0
#      print(c(p, v))
    }
}

# (1-Normality) t2
log_loops$norm_T2 <- numeric( N * length(emaLabels) )
for (p in 1:N){
    d <- filter(data, participantID==p, day.response %in% 51:100)
    for (v in seq_along(emaLabels)){
      if (length(unique(na.omit(d[ ,5+v]))) > 1) {
           log_loops$norm_T2[(p-1)*length(emaLabels)+v] <- 1-shapiro.test(d[ ,5+v])[[1]]
      } else
           log_loops$norm_T2[(p-1)*length(emaLabels)+v] <- 0
#      print(c(p, v))
    }
}

# rank stat
log_loops$rank_stat <- log_loops$completed_T1 * log_loops$completed_T2 * log_loops$norm_T1 * log_loops$norm_T2

# peak at rank statistics:
# log_loops %>% group_by(emaLabels) %>% summarise( avg = mean(rank_stat)) %>% View()

```


#### Network estimation

Idiographic network models will be estimated on an individual basis.
Contemporaneous and temporal (Lag-1) associations will be estimated in form of a Gaussian Graphical Model using the open-source R package *graphicalVAR* [@EpskampEtAl2018b].
<!-- cite R, todo: summarize how GVAR is estimated --> Lag-1 VAR models estimate temporal associations by predicting each variable by all variables in the network at the previous time point, including itself, using multivariate linear regression.
The tuning parameter $/gamma$, controlling the degree of regularization, will be set to 0.5 (default in R).
Should this result in predominantly empty networks, $/gamma$ will be reduced to 0.25 or 0.
All applied variations to $/gamma$ and their effects on the results will be reported.

<!-- Other methods to estimate idiographic network models exist, which each their own benefits. In contrast to multilevel network estimation, which may be preferred for it's ability to estimate indivududal networks while borrowing infomation from the group-level and requires less data points @EpskampEtAl2018b. However, in multilevel modeling, individual networks are restricted to have the same structure across individuals, which is an unlikely assumption in our heterogeneous sample.-->

<!--- recode network variables so that more score is worse (eg recode positive affect and daily functioning to reverse)
-->

```{r Composites}

# compute composite variables for positive affect, negative affect, and stress severity
data$PosAffect_sum <- data$alert + data$active + data$attentive + data$determined + data$inspired
data$NegAffect_sum <- data$afraid + data$nervous + data$hostile + data$ashamed + data$upset
data$StressSev_sum <- data$severe1 + data$severe2 + data$severe3 + data$severe4 + data$severe5 + data$severe6 + data$severe7

# TODO: should I use avearge scores instead? more common I guess
data$PosAffect_avg <- rowMeans( cbind( 
  data$alert, data$active, data$attentive, data$determined,  data$inspired ))
data$NegAffect_avg <- rowMeans( cbind( 
  data$afraid, data$nervous, data$hostile, data$ashamed, data$upset ))
data$StressSev_avg <- rowMeans( cbind( 
  data$severe1, data$severe2, data$severe3, data$severe4, data$severe5, data$severe6, data$severe7 ))

# TODO: reverse code positive affect for better visual interpretability? 

```

```{r NetworkEstimation}

# empty data frame for network output
networks <- data.frame(
           ID = rep(1:N, each = 2),
           Wave = rep(c("T1", "T2"), N),
           PCC_PA_NA = NA,
           PCC_PA_Stress = NA,
           PCC_PA_Task = NA,
           PCC_PA_Rank1 = NA,
           PCC_PA_Rank2 = NA,
           PCC_NA_Stress = NA,
           PCC_NA_tasks = NA,
           PCC_NA_Rank1 = NA,
           PCC_NA_Rank2 = NA,
           PCC_Stress_tasks = NA,
           PCC_Stress_Rank1 = NA,
           PCC_Stress_Ranks2 = NA,
           PCC_Tasks_Rank1 = NA,
           PCC_Tasks_Rank2 = NA,
           PCC_Rank1_Rank2 = NA,
           PDC_PA_PA = NA,
           PDC_PA_NA = NA,
           PDC_PA_Stress = NA,
           PDC_PA_Tasks = NA,
           PDC_PA_Rank1 = NA,
           PDC_PA_Rank2 = NA,
           PDC_NA_PA = NA,
           PDC_NA_NA = NA,
           PDC_NA_Stress = NA,
           PDC_NA_Tasks = NA,
           PDC_NA_Rank1 = NA,
           PDC_NA_Rank2 = NA,
           PDC_Stress_PA = NA,
           PDC_Stress_NA = NA,
           PDC_Stress_Stress = NA,
           PDC_Stress_Tasks = NA,
           PDC_Stress_Rank1 = NA,
           PDC_Stress_Rank2 = NA,
           PDC_Tasks_PA = NA,
           PDC_Tasks_NA = NA,
           PDC_Tasks_Stress = NA,
           PDC_Tasks_Tasks = NA,
           PDC_Tasks_Rank1 = NA,
           PDC_Tasks_Rank2 = NA,
           PDC_Rank1_PA = NA,
           PDC_Rank1_NA = NA,
           PDC_Rank1_Stress = NA,
           PDC_Rank1_Tasks = NA,
           PDC_Rank1_Rank1 = NA,
           PDC_Rank1_Rank2   = NA,
           PDC_Rank2_PA = NA,
           PDC_Rank2_NA = NA,
           PDC_Rank2_Stress = NA,
           PDC_Rank2_Tasks = NA,
           PDC_Rank2_Rank1 = NA,
           PDC_Rank2_Rank2 = NA,
           PDC_by_col  = NA,
           SumImputed = NA,# sum of imputed data points in T1 netw
           Rank1Var = NA, # selected 5th variable included in netw
           Rank2Var = NA, # selected 6th variable included in netw
           converged = NA, # FALSE = model did not converge
           empty = NA , # TRUE = estimated model is emmpty
           EBIC = NA,
           gamma = NA
)

# BeckJohnson2020 use gamma = 0, Lambda 0.025:0.25


for (p in 1:N){
  # filter data by participant
  d_T1 <- filter(data, participantID==p, day.response %in% 1:50)
  d_T2 <- filter(data, participantID==p, day.response %in% 51:100)
  #  extract two highest ranking DPDS variables, higher rank stat = better
  dpds  <- filter(log_loops, 
                  log_loops$ID == p, 
                  log_loops$emaLabels %in% 
                    grep("dpds", log_loops$emaLabels, value = TRUE)
                  )
  dpds_ranked <- dpds[order(dpds$rank_stat, decreasing = TRUE), ]
  Rank1       <- dpds_ranked[1, 3]
  Rank2       <- dpds_ranked[2, 3]
  
  # select variables for individualized network
  d_T1 <- select(d_T1, 
                 day.response, 
                 PosAffect_sum, 
                 NegAffect_sum, 
                 StressSev_sum, 
                 tasks, 
                 Rank1, 
                 Rank2
                 )
  d_T2 <- select(d_T2, 
                 day.response, 
                 PosAffect_sum, 
                 NegAffect_sum, 
                 StressSev_sum, 
                 tasks, 
                 Rank1, 
                 Rank2
                 ) 
  # BeckJohnson2020 used gamma = 0, Lambda 0.025:0.25
  netw_T1 <- graphicalVAR(d_T1, 
                          beepvar = "day.response",
                          gamma = 0,
                          lambda_beta = seq(.025, 1, .025)
                          )
  cat("<- netwT1 ", p)
  netw_T2 <- graphicalVAR(d_T2, 
                          beepvar = "day.response",
                          gamma = 0,
                          lambda_beta = seq(.025, 1, .025)
                          )
  cat("<- netwT2", p)
  
  assign(paste0("ID",p,"_T1"), netw_T1)
  assign(paste0("ID",p,"_T2"), netw_T2)

  networks[networks$ID == p, 3:17] <-
    rbind(as.vector(netw_T1$PCC[lower.tri(netw_T1$PCC)]), 
          as.vector(netw_T2$PCC[lower.tri(netw_T2$PCC)])
          )
  networks[networks$ID == p, 18:53] <- 
    rbind(as.vector(netw_T1$PDC),
          as.vector(netw_T2$PDC)
          )
}
# PPC:3:17, PDC 18:53
networks$pr_empty_PCC <- rowMeans(networks[ ,3:17] == 0)
networks$pr_empty_PDC <- rowMeans(networks[ ,18:53] == 0)

sum(networks$pr_empty_PCC == 1) # 1 empty contemp net total
sum(networks$pr_empty_PDC == 1) # 45 empty temp net total

comparisons <- data.frame(
  ID = 1:N,
  PCCs_cor = NA,
  PCCs_Jaccard = NA,
  PDCs_cor = NA,
  PDCs_Jaccard = NA
)

for (p in 1:N) {
  comparisons$PCCs_cor[p] <- cor(as.numeric(networks[(p-1)*2+p , 3:17]),
                  as.numeric(networks[(p-1)*2+p+1 , 3:17])
                  , method = "spearman")  # comparisons$PCCs_Jaccard <- 
  comparisons$PDCs_cor[p] <- cor(as.numeric(networks[(p-1)*2 + p, 18:53]),
                  as.numeric(networks[(p-1)*2 + p+1 , 18:53])
                  , method = "spearman")
}

hist(comparisons$PCCs_cor, breaks = 79)
hist(comparisons$PDCs_cor, breaks = 79)

# mean mode median, sd

####################
# extract metrics per network
# density
# proportion empty edges

# comparisons
# correlation adj matr PCC
# correlation adj matr PDC
# 
# prop of edges that are estimated as empty, positive or negative in both networks =
# jacqard similarity: (count of agreeing categories)/ (count of all edges)
# 
####################
# save network objects (for later), here("data",)
# assign(paste0("ID",p,"_T1"), netw_T1) #4.4mb per netw
# save RDS in subfolder using here()

# TODO build in option for empty networks and non-converging networks

# kappa matrix  is partial contemp corr PCC
# beta matrix is partical directed correlations PDC
# 
# compute comparison metric, see Mansueto code for examples
# vechs(cont) for contemp netw

# TODO plot networks
# Names for the Plot :
names <- c("PA", "NA", "Stress", "Tasks", toupper(Rank1), toupper(Rank2) )

# for plots, fix layout and node strength for comparability
# qgraph(netw_T1$PCC,  layout = "circle", nonsig = "hide", theme = "colorblind", title = "Contemporaneous", labels = names, vsize = 10,  asize = 10, mar = c(5,5,5,5)) 

```

#### Network comparisons

As an index of temporal stability, we compare idiographic networks estimated for the first and last 50 days of measurement by correlating estimated network edge weights.
Other conceptualizations and tests of network similarity have been proposed, the most popular one being the Network Comparison Test (NCT, @vanBorkuloEtAl2017 ).

<!-- Do we need Fishers R to Z transformation to transform correlations to normal distr for the regression? Beck does it in her Covid paper -->
<!-- when comparing the matrices, need to remove diagnonal!!! for contemporaneous netwokrs -->


<!-- perhaps only do contemporaneous networks for further 
for contemporaneous networks: 
library(OpenMx)
vechs(cont)

# temporal
as.vector(temporal)
-->

### Exploratory analysis of contributing factors:

The following baseline variables will be included tested as predictors: - Sex - Age - Income - past six months: Happy - past six months: Mobility - past six months: Impulse - past six months: Relationships - past six months: Work - past week: Suicidality - past year: Operation - Handicap - Cigarette - Alcohol - Substance - Time since last psychological treatment (including "never") - Treatment provider (as ordinal scale) - Comorbid / previously diagnosed depression - Comorbid / previously diagnosed Anxiety - Comorbid / previously diagnosed Substance abuse or other addiction (merge level 2 and 3, see codebook) - Comorbid / previously diagnosed Schizophrenia - Comorbid / previously diagnosed Eating Disorder - Relationship / Family problems - Life Satisfaction (Mean, Satisfaction with Life Scale) - Neuroticism (NEO-FFI) - Extraversion (NEO-FFI) - Openness (NEO-FFI) - Agreeableness (NEO-FFI) - Conscientiousness (NEO-FFI) The following statistical aspects will be included as predictors: - Total number of imputed data points per individual: Due to the imputation process, higher proportions of missing data may inflate estimates of temporal stability
<!-- perhaps include means and variances of items as well, but difficult to code.-->

## RQ2: LASSO predictive regression

To test which person-specific attributes or statistical factors predict temporal network stability, we use predictive regression with least absolute shrinkage and selection operator (LASSO, @Tibshirani1996, @McNeish2015 ) regularization.
In predictive regression, a model is optimized for it's ability to predict the outcome variable in a novel sample [@WestfallYarkoni2016].
This is in contrast to the more commonly used explanatory regression, in which model parameters are optimized to explain maximum variance in the observed sample, which can lead to overfitting and poor out-of sample utility.
LASSO regularization shrinks small beta coefficients to zero, effectively excluding less relevant predictors from the model.
The amount of shrinkage applied, determined by the tuning parameter lambda, is optimized for predictive accuracy using K-fold cross-validation.
Using LASSO regularization, we can explore a wide range of potential predictors while selecting a parsimonious model without relying om arbitrary significance cut-offs or prior theory for variable selection.
Data will be spit at random into a training set (80% of cases) and test set (20% of cases), fixing the random number generator at 1821 for reproducibility of the analysis.
The training set will be used for model estimation using K-fold cross validation with 5 folds.
Missing data (e.g. due to network model non-convergence or missing baseline variables) will be handled using listwise deletion.
The model with least prediction error during cross validation will be tested for out-of sample predictive accuracy in the test set.
If predictive accuracy is satisfactory (root mean squared error $<$.20, meaning predicted correlation coefficients are within +/- .10 of original correlation coefficient estimates 
<!--arbitrary value, but seemed reasonable: the ROT classifies degrees of correlation strength in steps of 20, so a deviation of 10 would lead to a similar interpretation in these terms)-->, the resulting model will be fit on the complete data set to extract unbiased parameter estimates and estimated variance explained.
<!-- feedback eiko: no threshold for interpreting model but do mention r^2 and out of sample pred acc-->

# Results


<!-- I could demonstrate 4 cases (eg. highly stable, highly instable, mostly empty, very dense network, and relate it to the metrics. explain what the metrics mean.
    ! when plotting, impose equal layout and node strength"maximum value of node strength", "identical layouts Fruchterman Reingold algorithm-->

# Discussion

### Implications

-   relate to stationarity assumption

-   relate to idea of monitoring change, critical slowing down, phase transitions, ROM

-   implications for feasibility of this approach for research and applications, ie power, study designs

-   propose ideas for further research on this

### Implications of high stability

-   assumption of stationarity realistic?

### Implications of low stability

-  underpowered (in)stability of estimates?

-   warrants caution regarding the inferences we draw (momentary impression of item correlations in certain period of time vs. stable process which extends beyond this period and could inform interventions (TODO: related to predictive value of networks?)

-   measurement error assumption: are we just capturing noise? (unlikely but still an issue to think about)

-   is extending measurement period instead of increasing frequency a good solution, eg.
    with planned missingness, or is non-stationarity and low replicability too much of concern?
    
<!-- mention time varying auto regressive network models @HaslbeckBringmanWaldorp2020 -->

### Limitations

- ACCURACY OF ESTIMATES UNKNOWN: Uncertainty around parameter estimates is unknown.We cannot know whether dissimilarity of networks is due to unreliable estimates (too low power, measurement error) or because underlying data-generating process changes (non-stationarity, important variables missing from network)

-   limitations of this data set for this research q (e.g. sample size / power)

-   constraints to generalizability (population, designs, time frame, estimation method)

-   We focus on similarity of global network structure, but there are many more ways to descibe and interpret networks which may or may not be relevant for replicability: network comparison test, predictive networks models, sensitivity and specificity of recovered edges if true network (assumed to be) known

-   Kalman imputation assumes MCAR, but likely there is some bias.

-   Important assumption made by (idiographic) network models is that constructs were measured without error. Little published research on this, but eg. SchreuderEtAl2020 assessed participants interpretation of EMA items over the course of 6 months and concluded interpretation was consistent. Changes in item interpretation also known as measurement invariance or response shift bias.

### Conclusion

\newpage

# References


\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

{\#refs custom-style="Bibliography"}

\endgroup
