# #select variables I want to use, save to publish along code
# data_daily    <- data %>% select(participantID:dpds32) %>% select(-sleep, -alcohol.daily, -drugs.daily, -starts_with("stress")) # daily
# data_baseline <- data %>% select(participantID, assessment.date:age, happy, swlsMean, recentTreatment, neoN:neoC) %>% distinct()
# write.csv(data_daily, here("data", "dd100_daily.csv"))
# write.csv(data_baseline, here("data", "dd100_baseline.csv"))
# rm(data)
data_daily    <- read.csv(here("data", "dd100_daily.csv"))    %>% select(-X)
data_baseline <- read.csv(here("data", "dd100_baseline.csv")) %>% select(-X)
# Chunk 3: exludeMissing
data <- data_daily
# count nr of missing per px
idx_missing <- matrix(NA, N, 7)
colnames(idx_missing) <- c("ID", "CompletedTotal", "CompletedT1", "CompletedT2", "Include", "Var_Tasks_T1", "Var_Tasks_T2")
for (p in 1:N){
# ID
idx_missing[p,1] <- p
# CompletedTotal
d <- filter(data, data$participantID == p)
idx_missing[p,2] <-  unique(d$total.days)
# CompletedT1
d1 <-  filter(d, day.response %in% 1:50, missingResponse == 1)
idx_missing[p,3] <- dim(d1)[1]
# CompletedT2
d2 <-  filter(d, day.response %in% 51:100, missingResponse == 1)
idx_missing[p,4] <- dim(d2)[1]
# also exclude participants with zero variance in 'tasks' variable
d3 <- filter(d, day.response %in% 1:50)
idx_missing[p,6] <- var(d3$tasks, na.rm = T) > 0
d4 <- filter(d, day.response %in% 51:100)
idx_missing[p,7] <- var(d4$tasks, na.rm = T) > 0
# Include; TRUE if more than 70 responses overall, and more than 35 both T1 and T2, and non-zero var in 'tasks'
idx_missing[p,5] <- idx_missing[p,2]>70 &
idx_missing[p,3]>35 &
idx_missing[p,4]>35 &
idx_missing[p,6]==T &
idx_missing[p,7]==T
}
# included IDs
idx_included <- idx_missing %>% as.data.frame() %>% filter(Include == TRUE)
# exclude participants
data <- filter(data, participantID %in% idx_included$ID)
# rename IDs for easy looping
data$participantID <- factor(data$participantID,
levels = unique(data$participantID),
labels = 1:length(unique(data$participantID))) %>%
as.numeric()
# N: nr participants retained
N <- max(unique(data$participantID)) # 79
data$PosAffect <- NA
data$NegAffect <- NA
data$StressSev <- NA
data_daily_raw <- data # keep copy of data after exclusion of participants
# Chunk 4: Kalman
#   apply Kalman filter to impute at level of participant, based on full 100d
emaLabels <- data %>% select(tasks:dpds32) %>% colnames()
library("imputeTS")
for (p in 1:N){
for (v in seq_along(emaLabels)){
if (length(unique(na.omit(data[data$participantID == p, 5+v]))) > 1){
data[data$participantID == p, 5+v] <- na_kalman(data[data$participantID == p, 5+v]) # throws error, see Note below
# cat("<- T ","ID",p,"var",v, sep = "")   # for check
} else  data[data$participantID == p, 5+v] <- unique(na.omit(data[data$participantID == p, 5+v]))
# cat("<- F","ID",p,"var",v, sep = "")    # for check
}
# compute composite variables for positive affect, negative affect, and stress severity
data$PosAffect <- (data$alert + data$active + data$attentive +
data$determined + data$inspired) / 5
data$NegAffect <- (data$afraid + data$nervous + data$hostile +
data$ashamed + data$upset) / 5
data$StressSev <- (data$severe1 + data$severe2 + data$severe3 +
data$severe4 + data$severe5 + data$severe6 + data$severe7) / 7
emaLabels <-  data %>%
select(-(participantID:total.days)) %>%
colnames()
data_daily_imputed <- data # keep copy of imputed data
# TODO: reverse code positive affect for better visual interpretability?
# NOTE:
# error is thrown when assigning imputed data to df:
#
# possible convergence problem: 'optim' gave code = 52
# and message ‘ERROR: ABNORMAL_TERMINATION_IN_LNSRCH’            [,1]      [,2]
#
# I checked the iterations with errors manually, the imputation and assignment seem to work despite error, so I continue.
#
# example of itiration w errors:
# i = 86; v = 41
# cbind( data_daily_imputed[data_daily_imputed$participantID == p, 5+v] , na_kalman(data[data_daily_imputed$participantID == p, 5+v]))
# Chunk 5: detrending
# We detrend at level of participant, using full 100d, regardless of significance level of linear trend
data <- data_daily_imputed
for (p in 1:N) {
for (v in seq_along(emaLabels)){
if ( data[data$participantID == p, 5+v] %>%
is.na() %>%
sum() ==
length(data[data$participantID == p, 5+v])) {next} #TODO: find more elegant solution to not evaluate composite scores...
formula <- as.formula(paste0(emaLabels[v], " ~ 1 + day.response")) # define formula for linear trend
trend_v <- lm(formula, data = data[data$participantID == p, ]) # fit model per person per var
data[data$participantID == p, 5+v] <- residuals(trend_v) # detrend
}
data_detrended <- data # keep copy of data after detrending
# NOTE:
# To check for significance of alpha, include below code in loop:
# if (anova(trend_v)[["Pr(>F)"]][1] < 0.05) { ... } else next
# Chunk 6: descriptives
# prepare data frame to store variable descriptives per person per time period
EMA_descriptives <- data.frame(
ID = rep(c(1:N), each = length(emaLabels)),
Var = rep(seq_along(emaLabels), N),
emaLabels = rep(c(emaLabels), N)
)
# function to filter rows for T1 and T2 data
.filterData <- function(timeperiod = c("T1", "T2"), dat, p){
if (timeperiod == "T1"){
t <- 1:50
} else if ( timeperiod == "T2"){
t <- 51:100
} else t <- 1:100
return( filter(dat, participantID == p, day.response %in% t) )
}
# extract variable descriptives
for (p in 1:N){
raw_T1        <- .filterData("T1", data_daily_raw, p)
raw_T2        <- .filterData("T2", data_daily_raw, p)
imputed_T1    <- .filterData("T1", data_daily_imputed, p)
imputed_T2    <- .filterData("T2", data_daily_imputed, p)
detrended_T1  <- .filterData("T1", data_detrended, p)
detrended_T2  <- .filterData("T2", data_detrended, p)
for (v in seq_along(emaLabels)){
# dynamic index for EMA_descriptives df
idx <- (p-1)*length(emaLabels)+v
# proportion completed T1 and T2
EMA_descriptives$completed_T1[idx] <- sum(!is.na(raw_T1[ ,5+v]))/50
EMA_descriptives$completed_T2[idx] <- sum(!is.na(raw_T2[ ,5+v]))/50
# (1-Normality) Raw Variables T1 and T2
if (length(unique(na.omit(raw_T1[ ,5+v]))) > 1) {
EMA_descriptives$norm_T1[idx] <- 1-shapiro.test(raw_T1[ ,5+v])[[1]]
} else  EMA_descriptives$norm_T1[idx] <- 0
if (length(unique(na.omit(raw_T2[ ,5+v]))) > 1) {
EMA_descriptives$norm_T2[idx] <- 1-shapiro.test(raw_T2[ ,5+v])[[1]]
} else  EMA_descriptives$norm_T2[idx] <- 0
# (1-Normality) Detrended Variables T1 and T2
if (length(unique(na.omit(raw_T1[ ,5+v]))) > 1) {
EMA_descriptives$norm_detrended_T1[idx] <- 1-shapiro.test(detrended_T1[ ,5+v])[[1]]
} else  EMA_descriptives$norm_detrended_T1[idx] <- 0
if (length(unique(na.omit(raw_T2[ ,5+v]))) > 1) {
EMA_descriptives$norm_detrended_T2[idx] <- 1-shapiro.test(detrended_T2[ ,5+v])[[1]]
} else  EMA_descriptives$norm_detrended_T2[idx] <- 0
# Means and SDs of raw and imputed, T1 and T2
EMA_descriptives$mean_raw_T1[idx]     <- raw_T1[ ,5+v]     %>% mean(na.rm = TRUE)
EMA_descriptives$mean_raw_T2[idx]     <- raw_T2[ ,5+v]     %>% mean(na.rm = TRUE)
EMA_descriptives$sd_raw_T1[idx]       <- raw_T1[ ,5+v]     %>% sd(na.rm = TRUE)
EMA_descriptives$sd_raw_T2[idx]       <- raw_T2[ ,5+v]     %>% sd(na.rm = TRUE)
EMA_descriptives$mean_imputed_T1[idx] <- imputed_T1[ ,5+v] %>% mean(na.rm = TRUE)
EMA_descriptives$mean_imputed_T2[idx] <- imputed_T2[ ,5+v] %>% mean(na.rm = TRUE)
EMA_descriptives$sd_imputed_T1[idx]   <- imputed_T1[ ,5+v] %>% sd(na.rm = TRUE)
EMA_descriptives$sd_imputed_T2[idx]   <- imputed_T2[ ,5+v] %>% sd(na.rm = TRUE)
}
# rank stat raw data
EMA_descriptives$rank_stat <- EMA_descriptives$completed_T1 *
EMA_descriptives$completed_T2 *
EMA_descriptives$norm_T1 *
EMA_descriptives$norm_T2
# rank stat on detrended data
EMA_descriptives$rank_stat_detrended <- EMA_descriptives$completed_T1 *
EMA_descriptives$completed_T2 *
EMA_descriptives$norm_detrended_T1 *
EMA_descriptives$norm_detrended_T2
saveRDS(EMA_descriptives, file = here("Data", "EMA_descriptives.RDS"))
# to peak at average rank score averages in sample:
# EMA_descriptives %>% group_by(emaLabels) %>% summarise( avg = mean(rank_stat_detrended)) %>% View()
# NOTE
# v = 1: first EMA Variable; located at 6th column in data (thus 5+v)
# ShapiroWilk test stat: 0 = perfect normality, 1 = deviation from normality.
# Because we want to prioritize more normally distributed variables, we use 1-ShapiroWilk.
# As result, a higher rank statistic means better properties.
# Chunk 1: setup
knitr::opts_chunk$set(warning = TRUE, message = TRUE)
# install.packages("here")
# setwd("~/GitHub/thesis_repo")
library("here")
here::i_am("ThesisAnalysisV2.Rmd") # find directory where file is in
# install.packages("conflicted")
library("conflicted")
conflict_prefer("filter", "dplyr")
# install.packages("tidyverse")
library("tidyverse")
# install.packages("papaja")
library("papaja")
# install.packages("imputeTS")
library("imputeTS")
# install.packages("graphicalVAR")
library("graphicalVAR")
# install.packages("qgraph")
library("qgraph")
# r_refs("r-references.bib")
# Notation:
# ---------
#     data:   full data set. Operations on the complete data set ()
#             In chunks that manipulate the whole data set (eg., imputation, detrending) I reassign the manipulated data object to 'data', so that results can easily be rerun on not detrended / not imputed data by simply not evaluating the previous code chunk
#     p:      participant (iteration in loop)
#     v:      variable (iteration in loop)
#     d:      temporary data object in loop
# sessionInfo()
# -------------
# R version 4.0.3 (2020-10-10)
# Platform: x86_64-apple-darwin17.0 (64-bit)
# Running under: macOS Big Sur 10.16
# Chunk 2: readData
# read data
# data <- read.csv(here("data", "dd100-proppert.csv"))
#
# N <- data$participantID %>% unique()
# range(N) # 1 : 116
# length(unique(N)) # 112
# setdiff(1:116, N) # Participants  8, 25, 45, 92  had been excluded by original authors because more than 30% of responses were missing
#
# # rename IDs for easier looping
# data$participantID <- factor(
#   data$participantID,
#   levels = unique(data$participantID),
#   labels = 1:length(unique(data$participantID))
#   ) %>%
#   as.numeric()
#
# #select variables I want to use, save to publish along code
# data_daily    <- data %>% select(participantID:dpds32) %>% select(-sleep, -alcohol.daily, -drugs.daily, -starts_with("stress")) # daily
# data_baseline <- data %>% select(participantID, assessment.date:age, happy, swlsMean, recentTreatment, neoN:neoC) %>% distinct()
# write.csv(data_daily, here("data", "dd100_daily.csv"))
# write.csv(data_baseline, here("data", "dd100_baseline.csv"))
# rm(data)
data_daily    <- read.csv(here("data", "dd100_daily.csv"))    %>% select(-X)
data_baseline <- read.csv(here("data", "dd100_baseline.csv")) %>% select(-X)
# Chunk 3: exludeMissing
data <- data_daily
# count nr of missing per px
idx_missing <- matrix(NA, N, 7)
colnames(idx_missing) <- c("ID", "CompletedTotal", "CompletedT1", "CompletedT2", "Include", "Var_Tasks_T1", "Var_Tasks_T2")
for (p in 1:N){
# ID
idx_missing[p,1] <- p
# CompletedTotal
d <- filter(data, data$participantID == p)
idx_missing[p,2] <-  unique(d$total.days)
# CompletedT1
d1 <-  filter(d, day.response %in% 1:50, missingResponse == 1)
idx_missing[p,3] <- dim(d1)[1]
# CompletedT2
d2 <-  filter(d, day.response %in% 51:100, missingResponse == 1)
idx_missing[p,4] <- dim(d2)[1]
# also exclude participants with zero variance in 'tasks' variable
d3 <- filter(d, day.response %in% 1:50)
idx_missing[p,6] <- var(d3$tasks, na.rm = T) > 0
d4 <- filter(d, day.response %in% 51:100)
idx_missing[p,7] <- var(d4$tasks, na.rm = T) > 0
# Include; TRUE if more than 70 responses overall, and more than 35 both T1 and T2, and non-zero var in 'tasks'
idx_missing[p,5] <- idx_missing[p,2]>70 &
idx_missing[p,3]>35 &
idx_missing[p,4]>35 &
idx_missing[p,6]==T &
idx_missing[p,7]==T
}
# included IDs
idx_included <- idx_missing %>% as.data.frame() %>% filter(Include == TRUE)
# exclude participants
data <- filter(data, participantID %in% idx_included$ID)
# rename IDs for easy looping
data$participantID <- factor(data$participantID,
levels = unique(data$participantID),
labels = 1:length(unique(data$participantID))) %>%
as.numeric()
# N: nr participants retained
N <- max(unique(data$participantID)) # 79
data$PosAffect <- NA
data$NegAffect <- NA
data$StressSev <- NA
data_daily_raw <- data # keep copy of data after exclusion of participants
# Chunk 4: Kalman
#   apply Kalman filter to impute at level of participant, based on full 100d
emaLabels <- data %>% select(tasks:dpds32) %>% colnames()
library("imputeTS")
for (p in 1:N){
for (v in seq_along(emaLabels)){
if (length(unique(na.omit(data[data$participantID == p, 5+v]))) > 1){
data[data$participantID == p, 5+v] <- na_kalman(data[data$participantID == p, 5+v]) # throws error, see Note below
# cat("<- T ","ID",p,"var",v, sep = "")   # for check
} else  data[data$participantID == p, 5+v] <- unique(na.omit(data[data$participantID == p, 5+v]))
# cat("<- F","ID",p,"var",v, sep = "")    # for check
}
# compute composite variables for positive affect, negative affect, and stress severity
data$PosAffect <- (data$alert + data$active + data$attentive +
data$determined + data$inspired) / 5
data$NegAffect <- (data$afraid + data$nervous + data$hostile +
data$ashamed + data$upset) / 5
data$StressSev <- (data$severe1 + data$severe2 + data$severe3 +
data$severe4 + data$severe5 + data$severe6 + data$severe7) / 7
emaLabels <-  data %>%
select(-(participantID:total.days)) %>%
colnames()
data_daily_imputed <- data # keep copy of imputed data
# TODO: reverse code positive affect for better visual interpretability?
# NOTE:
# error is thrown when assigning imputed data to df:
#
# possible convergence problem: 'optim' gave code = 52
# and message ‘ERROR: ABNORMAL_TERMINATION_IN_LNSRCH’            [,1]      [,2]
#
# I checked the iterations with errors manually, the imputation and assignment seem to work despite error, so I continue.
#
# example of itiration w errors:
# i = 86; v = 41
# cbind( data_daily_imputed[data_daily_imputed$participantID == p, 5+v] , na_kalman(data[data_daily_imputed$participantID == p, 5+v]))
# Chunk 5: detrending
# We detrend at level of participant, using full 100d, regardless of significance level of linear trend
data <- data_daily_imputed
for (p in 1:N) {
for (v in seq_along(emaLabels)){
if ( data[data$participantID == p, 5+v] %>%
is.na() %>%
sum() ==
length(data[data$participantID == p, 5+v])) {next} #TODO: find more elegant solution to not evaluate composite scores...
formula <- as.formula(paste0(emaLabels[v], " ~ 1 + day.response")) # define formula for linear trend
trend_v <- lm(formula, data = data[data$participantID == p, ]) # fit model per person per var
data[data$participantID == p, 5+v] <- residuals(trend_v) # detrend
}
data_detrended <- data # keep copy of data after detrending
# NOTE:
# To check for significance of alpha, include below code in loop:
# if (anova(trend_v)[["Pr(>F)"]][1] < 0.05) { ... } else next
# Chunk 6: descriptives
# prepare data frame to store variable descriptives per person per time period
EMA_descriptives <- data.frame(
ID = rep(c(1:N), each = length(emaLabels)),
Var = rep(seq_along(emaLabels), N),
emaLabels = rep(c(emaLabels), N)
)
# function to filter rows for T1 and T2 data
.filterData <- function(timeperiod = c("T1", "T2"), dat, p){
if (timeperiod == "T1"){
t <- 1:50
} else if ( timeperiod == "T2"){
t <- 51:100
} else t <- 1:100
return( filter(dat, participantID == p, day.response %in% t) )
}
# extract variable descriptives
for (p in 1:N){
raw_T1        <- .filterData("T1", data_daily_raw, p)
raw_T2        <- .filterData("T2", data_daily_raw, p)
imputed_T1    <- .filterData("T1", data_daily_imputed, p)
imputed_T2    <- .filterData("T2", data_daily_imputed, p)
detrended_T1  <- .filterData("T1", data_detrended, p)
detrended_T2  <- .filterData("T2", data_detrended, p)
for (v in seq_along(emaLabels)){
# dynamic index for EMA_descriptives df
idx <- (p-1)*length(emaLabels)+v
# proportion completed T1 and T2
EMA_descriptives$completed_T1[idx] <- sum(!is.na(raw_T1[ ,5+v]))/50
EMA_descriptives$completed_T2[idx] <- sum(!is.na(raw_T2[ ,5+v]))/50
# (1-Normality) Raw Variables T1 and T2
if (length(unique(na.omit(raw_T1[ ,5+v]))) > 1) {
EMA_descriptives$norm_T1[idx] <- 1-shapiro.test(raw_T1[ ,5+v])[[1]]
} else  EMA_descriptives$norm_T1[idx] <- 0
if (length(unique(na.omit(raw_T2[ ,5+v]))) > 1) {
EMA_descriptives$norm_T2[idx] <- 1-shapiro.test(raw_T2[ ,5+v])[[1]]
} else  EMA_descriptives$norm_T2[idx] <- 0
# (1-Normality) Detrended Variables T1 and T2
if (length(unique(na.omit(raw_T1[ ,5+v]))) > 1) {
EMA_descriptives$norm_detrended_T1[idx] <- 1-shapiro.test(detrended_T1[ ,5+v])[[1]]
} else  EMA_descriptives$norm_detrended_T1[idx] <- 0
if (length(unique(na.omit(raw_T2[ ,5+v]))) > 1) {
EMA_descriptives$norm_detrended_T2[idx] <- 1-shapiro.test(detrended_T2[ ,5+v])[[1]]
} else  EMA_descriptives$norm_detrended_T2[idx] <- 0
# Means and SDs of raw and imputed, T1 and T2
EMA_descriptives$mean_raw_T1[idx]     <- raw_T1[ ,5+v]     %>% mean(na.rm = TRUE)
EMA_descriptives$mean_raw_T2[idx]     <- raw_T2[ ,5+v]     %>% mean(na.rm = TRUE)
EMA_descriptives$sd_raw_T1[idx]       <- raw_T1[ ,5+v]     %>% sd(na.rm = TRUE)
EMA_descriptives$sd_raw_T2[idx]       <- raw_T2[ ,5+v]     %>% sd(na.rm = TRUE)
EMA_descriptives$mean_imputed_T1[idx] <- imputed_T1[ ,5+v] %>% mean(na.rm = TRUE)
EMA_descriptives$mean_imputed_T2[idx] <- imputed_T2[ ,5+v] %>% mean(na.rm = TRUE)
EMA_descriptives$sd_imputed_T1[idx]   <- imputed_T1[ ,5+v] %>% sd(na.rm = TRUE)
EMA_descriptives$sd_imputed_T2[idx]   <- imputed_T2[ ,5+v] %>% sd(na.rm = TRUE)
}
# rank stat raw data
EMA_descriptives$rank_stat <- EMA_descriptives$completed_T1 *
EMA_descriptives$completed_T2 *
EMA_descriptives$norm_T1 *
EMA_descriptives$norm_T2
# rank stat on detrended data
EMA_descriptives$rank_stat_detrended <- EMA_descriptives$completed_T1 *
EMA_descriptives$completed_T2 *
EMA_descriptives$norm_detrended_T1 *
EMA_descriptives$norm_detrended_T2
saveRDS(EMA_descriptives, file = here("Data", "EMA_descriptives.RDS"))
# to peak at average rank score averages in sample:
# EMA_descriptives %>% group_by(emaLabels) %>% summarise( avg = mean(rank_stat_detrended)) %>% View()
# NOTE
# v = 1: first EMA Variable; located at 6th column in data (thus 5+v)
# ShapiroWilk test stat: 0 = perfect normality, 1 = deviation from normality.
# Because we want to prioritize more normally distributed variables, we use 1-ShapiroWilk.
# As result, a higher rank statistic means better properties.
# empty data frame to store path estimates in long format (two rows per participant, 1 for T1, 1 for T2)
networks <- data.frame("ID" = rep(1:N, each=2),
"timeperiod" = rep(c("T1", "T2"), N),
# 3:17 Contemporaneous network: lower triangle in col by col format
"PCC_PA_NA" = NA,
"PCC_PA_Stress" = NA,
"PCC_PA_Task" = NA,
"PCC_PA_Rank1" = NA,
"PCC_PA_Rank2" = NA,
"PCC_NA_Stress" = NA,
"PCC_NA_tasks" = NA,
"PCC_NA_Rank1" = NA,
"PCC_NA_Rank2" = NA,
"PCC_Stress_tasks" = NA,
"PCC_Stress_Rank1" = NA,
"PCC_Stress_Ranks2" = NA,
"PCC_Tasks_Rank1" = NA,
"PCC_Tasks_Rank2" = NA,
"PCC_Rank1_Rank2" = NA,
# 18:53 Temporal networks: full matrix in col by col format
"PDC_PA_PA" = NA,
"PDC_PA_NA" = NA,
"PDC_PA_Stress" = NA,
"PDC_PA_Tasks" = NA,
"PDC_PA_Rank1" = NA,
"PDC_PA_Rank2" = NA,
"PDC_NA_PA" = NA,
"PDC_NA_NA" = NA,
"PDC_NA_Stress" = NA,
"PDC_NA_Tasks" = NA,
"PDC_NA_Rank1" = NA,
"PDC_NA_Rank2" = NA,
"PDC_Stress_PA" = NA,
"PDC_Stress_NA" = NA,
"PDC_Stress_Stress" = NA,
"PDC_Stress_Tasks" = NA,
"PDC_Stress_Rank1" = NA,
"PDC_Stress_Rank2" = NA,
"PDC_Tasks_PA" = NA,
"PDC_Tasks_NA" = NA,
"PDC_Tasks_Stress" = NA,
"PDC_Tasks_Tasks" = NA,
"PDC_Tasks_Rank1" = NA,
"PDC_Tasks_Rank2" = NA,
"PDC_Rank1_PA" = NA,
"PDC_Rank1_NA" = NA,
"PDC_Rank1_Stress" = NA,
"PDC_Rank1_Tasks" = NA,
"PDC_Rank1_Rank1" = NA,
"PDC_Rank1_Rank2"   = NA,
"PDC_Rank2_PA" = NA,
"PDC_Rank2_NA" = NA,
"PDC_Rank2_Stress" = NA,
"PDC_Rank2_Tasks" = NA,
"PDC_Rank2_Rank1" = NA,
"PDC_Rank2_Rank2" = NA
)
# index columns for contemporaneous network (PPC: 3:17) and temporal networks (PDC 18:53) in Networks df
idx_PCC <- 3:17
idx_PDC <- 18:53
# data frame for main outcome statistics
comparisons <- data.frame(ID = 1:N)
p=2
# if (p %in% c(2)) { # model not converging for Participant 2
#  next
#  }
# filter data by participant
d_T1 <- filter(data, participantID==p, day.response %in% 1:50)
d_T2 <- filter(data, participantID==p, day.response %in% 51:100)
#  select two highest ranking DPDS/behavior variables, higher rank stat = better
selectVar  <- filter(EMA_descriptives,
EMA_descriptives$ID == p,
EMA_descriptives$emaLabels %in%
emaLabels[c(11:23, 31:60)]
)
selectVar <- selectVar[order(selectVar$rank_stat_detrended, decreasing = TRUE), ]
Rank1       <- selectVar[1, 3]
Rank2       <- selectVar[2, 3]
# log name of DPDS Variables
comparisons$Rank1[p] <- Rank1
comparisons$Rank2[p] <- Rank2
# define individualized networks
d_T1 <- select(d_T1,
day.response,
PosAffect,
NegAffect,
StressSev,
tasks,
all_of(Rank1),
all_of(Rank2)
)
d_T2 <- select(d_T2,
day.response,
PosAffect,
NegAffect,
StressSev,
tasks,
all_of(Rank1),
all_of(Rank2)
)
# BeckJohnson2020 used gamma = 0, Lambda 0.025:1 by 0.025
netw_T1 <- graphicalVAR(d_T1,
beepvar = "day.response",
gamma = 0,
lambda_beta = seq(.025, 1, .0125)
)
netw_T2 <- graphicalVAR(d_T2,
beepvar = "day.response",
gamma = 0,
lambda_beta = seq(.025, 1, .0125)
)
